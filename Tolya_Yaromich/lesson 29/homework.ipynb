{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–∞–Ω–Ω–æ–º –∑–∞–¥–∞–Ω–∏–∏ –º–Ω–µ –∑–∞—Ö–æ—Ç–µ–ª–æ—Å—å –¥–æ–æ–±—É—á–∏—Ç—å  –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–æ–µ–π –ø–µ—Ä–µ–ø–∏—Å–∫–µ –∏–∑ —Ç–µ–ª–µ–≥—Ä–∞–º–∞ —Å–æ —Å–≤–æ–∏–º–∏ –¥—Ä—É–∑—å—è–º–∏,–≤ –∏—Ç–æ–≥–µ –≤—ã—à–µ–ª –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ –¥–∞ 10000 —Ñ—Ä–∞–∑ —Å–æ–≤–º–µ—Å—Ç–æ –º–µ–Ω—è –∏ –¥—Ä—É–∑–µ–π.\n",
    "–≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–∫–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —è –≤–∑—è–ª gpt2 –∏ –ø–æ–ø—ã—Ç–∞—é—Å—å –µ–µ –¥–æ–æ–±—É—á–∏—Ç—å –Ω–∞ —Å–≤–æ–∏—Ö —Ñ—Ä–∞–∑–∞—Ö —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–æ–ø—Ä–æ–±–æ–≤–∞–ª–∞ –ø–æ–¥—Ä–∞–∂–∞—Ç—å –º–æ–µ–º—É —Å—Ç–∏–ª—é –æ–±—â–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from keras import models\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.metrics import AUC\n",
    "from keras.regularizers import L1\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import torch\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print('Cuda version: ' + tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_test =\"./dataset_token_labels.csv\"\n",
    "tokenized_datasets = pd.read_csv(file_path_test, encoding='utf-8')\n",
    "tokenized_datasets.drop(columns=['tokenized_reviews', 'tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        name           type         id  \\\n",
      "0       V14D  personal_chat  200693762   \n",
      "1       V14D  personal_chat  200693762   \n",
      "2       V14D  personal_chat  200693762   \n",
      "3       V14D  personal_chat  200693762   \n",
      "4       V14D  personal_chat  200693762   \n",
      "...      ...            ...        ...   \n",
      "91578  V3N1K  personal_chat  360552278   \n",
      "91579  V3N1K  personal_chat  360552278   \n",
      "91580  V3N1K  personal_chat  360552278   \n",
      "91581  V3N1K  personal_chat  360552278   \n",
      "91582  V3N1K  personal_chat  360552278   \n",
      "\n",
      "                                                messages  \n",
      "0      {'id': 498643, 'type': 'service', 'date': '202...  \n",
      "1      {'id': 498644, 'type': 'message', 'date': '202...  \n",
      "2      {'id': 498645, 'type': 'message', 'date': '202...  \n",
      "3      {'id': 498646, 'type': 'service', 'date': '202...  \n",
      "4      {'id': 498647, 'type': 'message', 'date': '202...  \n",
      "...                                                  ...  \n",
      "91578  {'id': 778920, 'type': 'message', 'date': '202...  \n",
      "91579  {'id': 778921, 'type': 'message', 'date': '202...  \n",
      "91580  {'id': 778922, 'type': 'message', 'date': '202...  \n",
      "91581  {'id': 778923, 'type': 'message', 'date': '202...  \n",
      "91582  {'id': 778934, 'type': 'message', 'date': '202...  \n",
      "\n",
      "[91583 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# –°–æ–∑–¥–∞–π—Ç–µ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–æ–≤\n",
    "dataframes = []\n",
    "\n",
    "# –ü–µ—Ä–µ–±–µ—Ä–∏—Ç–µ –∫–∞–∂–¥—ã–π JSON —Ñ–∞–π–ª –≤ –≤–∞—à–µ–º –∫–∞—Ç–∞–ª–æ–≥–µ\n",
    "json_files = [file for file in os.listdir('.') if file.endswith('.json')]\n",
    "for file in json_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        # –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞\n",
    "        data = json.load(f)\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ DataFrame –∏ –¥–æ–±–∞–≤—å—Ç–µ –∏—Ö –≤ —Å–ø–∏—Å–æ–∫\n",
    "        df = pd.DataFrame(data)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–∏—Ç–µ –≤—Å–µ DataFrame –≤ –æ–¥–∏–Ω\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –æ–¥–∏–Ω Pandas DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤\n",
    "print(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_88918_row0_col0, #T_88918_row0_col1, #T_88918_row0_col2, #T_88918_row0_col3, #T_88918_row1_col0, #T_88918_row1_col1, #T_88918_row1_col2, #T_88918_row1_col3, #T_88918_row2_col0, #T_88918_row2_col1, #T_88918_row2_col2, #T_88918_row2_col3, #T_88918_row3_col0, #T_88918_row3_col1, #T_88918_row3_col2, #T_88918_row3_col3, #T_88918_row4_col0, #T_88918_row4_col1, #T_88918_row4_col2, #T_88918_row4_col3 {\n",
       "  width: 300px;\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_88918\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_88918_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n",
       "      <th id=\"T_88918_level0_col1\" class=\"col_heading level0 col1\" >type</th>\n",
       "      <th id=\"T_88918_level0_col2\" class=\"col_heading level0 col2\" >id</th>\n",
       "      <th id=\"T_88918_level0_col3\" class=\"col_heading level0 col3\" >messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_88918_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_88918_row0_col0\" class=\"data row0 col0\" >V14D</td>\n",
       "      <td id=\"T_88918_row0_col1\" class=\"data row0 col1\" >personal_chat</td>\n",
       "      <td id=\"T_88918_row0_col2\" class=\"data row0 col2\" >200693762</td>\n",
       "      <td id=\"T_88918_row0_col3\" class=\"data row0 col3\" >{'id': 498643, 'type': 'service', 'date': '2020-09-07T19:53:42', 'date_unixtime': '1599497622', 'actor': 'Tos', 'actor_id': 'user368720613', 'action': 'clear_history', 'text': '', 'text_entities': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88918_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_88918_row1_col0\" class=\"data row1 col0\" >V14D</td>\n",
       "      <td id=\"T_88918_row1_col1\" class=\"data row1 col1\" >personal_chat</td>\n",
       "      <td id=\"T_88918_row1_col2\" class=\"data row1 col2\" >200693762</td>\n",
       "      <td id=\"T_88918_row1_col3\" class=\"data row1 col3\" >{'id': 498644, 'type': 'message', 'date': '2020-09-07T19:55:36', 'date_unixtime': '1599497736', 'from': 'Tos', 'from_id': 'user368720613', 'text': '–ú–æ—â–Ω–æ', 'text_entities': [{'type': 'plain', 'text': '–ú–æ—â–Ω–æ'}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88918_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_88918_row2_col0\" class=\"data row2 col0\" >V14D</td>\n",
       "      <td id=\"T_88918_row2_col1\" class=\"data row2 col1\" >personal_chat</td>\n",
       "      <td id=\"T_88918_row2_col2\" class=\"data row2 col2\" >200693762</td>\n",
       "      <td id=\"T_88918_row2_col3\" class=\"data row2 col3\" >{'id': 498645, 'type': 'message', 'date': '2020-09-07T19:57:03', 'date_unixtime': '1599497823', 'from': 'Tos', 'from_id': 'user368720613', 'text': '–í–ª–∞–¥?', 'text_entities': [{'type': 'plain', 'text': '–í–ª–∞–¥?'}]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88918_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_88918_row3_col0\" class=\"data row3 col0\" >V14D</td>\n",
       "      <td id=\"T_88918_row3_col1\" class=\"data row3 col1\" >personal_chat</td>\n",
       "      <td id=\"T_88918_row3_col2\" class=\"data row3 col2\" >200693762</td>\n",
       "      <td id=\"T_88918_row3_col3\" class=\"data row3 col3\" >{'id': 498646, 'type': 'service', 'date': '2020-09-07T20:00:44', 'date_unixtime': '1599498044', 'actor': 'Tos', 'actor_id': 'user368720613', 'action': 'phone_call', 'discard_reason': 'busy', 'text': '', 'text_entities': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_88918_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_88918_row4_col0\" class=\"data row4 col0\" >V14D</td>\n",
       "      <td id=\"T_88918_row4_col1\" class=\"data row4 col1\" >personal_chat</td>\n",
       "      <td id=\"T_88918_row4_col2\" class=\"data row4 col2\" >200693762</td>\n",
       "      <td id=\"T_88918_row4_col3\" class=\"data row4 col3\" >{'id': 498647, 'type': 'message', 'date': '2020-09-07T20:10:09', 'date_unixtime': '1599498609', 'from': 'V14D', 'from_id': 'user200693762', 'text': '–ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?', 'text_entities': [{'type': 'plain', 'text': '–ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?'}]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x18584146b90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–≤–æ–¥–∏–º—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# –í—ã–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ final_df.head() –≤ —à–∏—Ä–∏–Ω—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
    "final_df.head().style.set_properties(**{'width': '300px', 'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'messages'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –≤–∞—à DataFrame –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è final_df\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è \"actor\" –∏ \"text\" –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü \"messages\", —Ç–∞–∫ –∫–∞–∫ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'messages'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ –≤–∞—à DataFrame –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è final_df\n",
    "\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è \"actor\" –∏ \"text\" –∏–∑ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è\n",
    "final_df['actor'] = final_df['messages'].apply(lambda x: x.get('from', ''))\n",
    "final_df['text'] = final_df['messages'].apply(lambda x: x.get('text', ''))\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±–µ—Ü \"messages\", —Ç–∞–∫ –∫–∞–∫ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω\n",
    "final_df.drop(columns=['messages'], inplace=True)\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
    "print(final_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'messages'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'messages'"
     ]
    }
   ],
   "source": [
    "print(final_df['messages'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>actor</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V14D</td>\n",
       "      <td>personal_chat</td>\n",
       "      <td>200693762</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V14D</td>\n",
       "      <td>personal_chat</td>\n",
       "      <td>200693762</td>\n",
       "      <td>Tos</td>\n",
       "      <td>–ú–æ—â–Ω–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V14D</td>\n",
       "      <td>personal_chat</td>\n",
       "      <td>200693762</td>\n",
       "      <td>Tos</td>\n",
       "      <td>–í–ª–∞–¥?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V14D</td>\n",
       "      <td>personal_chat</td>\n",
       "      <td>200693762</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V14D</td>\n",
       "      <td>personal_chat</td>\n",
       "      <td>200693762</td>\n",
       "      <td>V14D</td>\n",
       "      <td>–ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name           type         id actor                  text\n",
       "0  V14D  personal_chat  200693762                            \n",
       "1  V14D  personal_chat  200693762   Tos                 –ú–æ—â–Ω–æ\n",
       "2  V14D  personal_chat  200693762   Tos                 –í–ª–∞–¥?\n",
       "3  V14D  personal_chat  200693762                            \n",
       "4  V14D  personal_chat  200693762  V14D  –ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(columns=['name','type','id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tos' '' 'V14D' 'Xolod 2.0' 'IV4N0V']\n"
     ]
    }
   ],
   "source": [
    "actor_values = final_df['actor'].unique()\n",
    "\n",
    "# –í—ã–≤–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "print(actor_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(final_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('messager.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_test =\"./messager.csv\"\n",
    "df = pd.read_csv(file_path_test, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tos</td>\n",
       "      <td>–ú–æ—â–Ω–æ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tos</td>\n",
       "      <td>–í–ª–∞–¥?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V14D</td>\n",
       "      <td>–ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>V14D</td>\n",
       "      <td>–Ø –Ω–µ –ø–æ–Ω–∏–º–∞–±</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>V14D</td>\n",
       "      <td>–Ω—É –æ–∫</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actor                  text\n",
       "1   Tos                 –ú–æ—â–Ω–æ\n",
       "2   Tos                 –í–ª–∞–¥?\n",
       "4  V14D  –ß—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å –¢–æ–ª—è ?\n",
       "5  V14D          –Ø –Ω–µ –ø–æ–Ω–∏–º–∞–±\n",
       "6  V14D                 –Ω—É –æ–∫"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df_clean[\"text\"]\n",
    "text.to_csv('messager2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9b8a8aea2547279febbfe7743b9a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc8543dd74647a3a9a06a095cd15579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342655f0670d45db9d7256d4a3270ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe79b5117b04028b741560d0c95c9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d45a57815549c2b857916adf31f1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfb88362f0640bcb45328a5c7a3579b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ casual language modeling\n",
    "model_name = \"GPT2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_clean[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12889e16decc4a55a43d69c992b759df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d3881c7d1a4c708692af23804f90dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Dmitriy007/rugpt2_gen_news\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Dmitriy007/rugpt2_gen_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—Å–Ω–∏–∑—É –∫–æ–¥ –∏–∑ hugging face –ø–æ —Ç–æ–º—É –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –Ω–µ –¥–æ–æ–±—É—á–∞—è –µ–µ ,–æ–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "\n",
    "input_text = '–£—á–µ–Ω–∏–∫ —Å—Ç–∞—Ä—à–µ–≥–æ –∫–ª–∞—Å—Å–∞ –ª–∏—Ü–µ—è ‚Ññ 21 –ò–≤–∞–Ω –°–∏–¥–æ—Ä–æ–≤ –∏–∑ –≥–æ—Ä–æ–¥–∞ –ê–¥–ª–µ—Ä –ø–æ–ª–µ—Ç–µ–ª –≤ –∫–æ—Å–º–æ—Å –Ω–∞ –ø–ª–∞–Ω–µ—Ç—É –ú–∞—Ä—Å.'\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "model.to('cuda')\n",
    "inputs.to('cuda')\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "output = model.generate(\n",
    "input_ids,\n",
    "attention_mask=inputs[\"attention_mask\"],\n",
    "pad_token_id=model.config.bos_token_id,\n",
    "max_length=300,\n",
    "num_beams=5,\n",
    "num_return_sequences=1,\n",
    "top_k=50,\n",
    "top_p=0.90,\n",
    "no_repeat_ngram_size=2,\n",
    "temperature=0.7,\n",
    "early_stopping=True\n",
    ")\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, output))\n",
    "print(generated_text[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "c:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d54b40eb72497ba374c6e596a68a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2541, training_loss=3.435218471750787, metrics={'train_runtime': 992.1285, 'train_samples_per_second': 20.489, 'train_steps_per_second': 2.561, 'train_loss': 3.435218471750787, 'epoch': 3.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dmitriy007/rugpt2_gen_news\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Dmitriy007/rugpt2_gen_news\")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./messager2.csv\",  # –ü—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "    block_size=128  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    ")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–π–Ω–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./model2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Dmitriy007/rugpt2_gen_news\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./model2\")\n",
    "\n",
    "# —ç—Ç–æ –ø—Ä–æ–º–ø—Ç\n",
    "input_text = \"–≠—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è \n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# –ü–µ—Ä–µ–¥–∞—á–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "inputs.to(device)\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=300,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∂–∏–º —Å –≤—ã–±–æ—Ä–∫–æ–π\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_beams=5,  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á–µ–π –±–æ–ª—å—à–µ 1\n",
    "    early_stopping=False  # –û—Ç–∫–ª—é—á–∏—Ç—å —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É\n",
    ")\n",
    "\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–≠—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,—ç—Ç–æ —Å—Ç—Ä–∞–Ω–Ω–æ\"\\n\"–î–∞,'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù—É –æ–Ω –æ–ø—Ä–µ–¥–µ–ª–Ω–Ω–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—á—Ç–æ –ø–æ—Ö–æ–∂–µ–µ,–Ω–∞ –ø—Ä–∞–≤–¥—É –Ω–æ –º–Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è—Ç–Ω–æ –ø–æ—á–µ–º—É –≤ —Ç–∞–∫–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='Dmitriy007/rugpt2_gen_news', vocab_size=50257, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50257: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "tokenizer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "22f30260a2b4101620faa352ff4158c8c03486483c8393b76c92b58694587b7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
