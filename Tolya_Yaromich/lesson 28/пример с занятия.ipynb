{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\anon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv('SPAM.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category                                            Message\n",
       "0         ham  Go until jurong point, crazy.. Available only ...\n",
       "1         ham                      Ok lar... Joking wif u oni...\n",
       "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         ham  U dun say so early hor... U c already then say...\n",
       "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...       ...                                                ...\n",
       "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568      ham               Will ü b going to esplanade fr home?\n",
       "5569      ham  Pity, * was in mood for that. So...any other s...\n",
       "5570      ham  The guy did some bitching but I acted like i'd...\n",
       "5571      ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace='None', value=np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS  = stopwords.words('english') + ['im']\n",
    "STOPWORDS.remove('not')\n",
    "STOPWORDS.remove('no')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#### Функция очистки и лемматизации текста\n",
    "def clean_text(text):\n",
    "  #очистить текст от тегов html и привести к нижнему регистру\n",
    "  text = BeautifulSoup(text).get_text().lower()\n",
    "\n",
    "  #очистить текст от ссылок html\n",
    "  #text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "\n",
    "  #очистить текст от всех символов кроме букв латинского алфавита\n",
    "  #text = re.sub(\"[^a-z]\", \" \", text)\n",
    "\n",
    "  #разбить текст на слова\n",
    "  #text_words = text.split()\n",
    "\n",
    "  #выполнить лематизацию текста и удалить стоп-слова\n",
    "  #result_words = [lemmatizer.lemmatize(word) for word in text_words if not word in STOPWORDS]\n",
    "    \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anon\\AppData\\Local\\Temp\\ipykernel_652\\3177496906.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text).get_text().lower()\n"
     ]
    }
   ],
   "source": [
    "df['Message'] = df['Message'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point, crazy.. available only ...\n",
       "1                           ok lar... joking wif u oni...\n",
       "2       free entry in 2 a wkly comp to win fa cup fina...\n",
       "3       u dun say so early hor... u c already then say...\n",
       "4       nah i don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    this is the 2nd time we have tried 2 contact u...\n",
       "5568                 will ü b going to esplanade fr home?\n",
       "5569    pity, * was in mood for that. so...any other s...\n",
       "5570    the guy did some bitching but i acted like i'd...\n",
       "5571                           rofl. its true to its name\n",
       "Name: Message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labeles = LabelEncoder()\n",
    "labels = labeles.fit_transform(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeles.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_PROCS = 32\n",
    "LR = 0.00005\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "   0:'ham', 1:'spam'\n",
    "}\n",
    "label2id = {\n",
    "   0:'ham', 1:'spam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(queries, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Message'], labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# classes = np.unique(y)\n",
    "# class_weights = compute_class_weight('balanced', classes=classes, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "classes = np.unique(labels)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Преобразуйте веса классов в тензор PyTorch\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "data_test = pd.DataFrame({'text': X_test, 'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    966\n",
       "1    149\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_1 = data_test[data_test['label'] == 1]\n",
    "\n",
    "# Выберите 10% строк, где label=0\n",
    "df_label_0 = data_test[data_test['label'] == 0].sample(frac=0.05)\n",
    "\n",
    "# Объедините два DataFrame\n",
    "new_df = pd.concat([df_label_1, df_label_0])\n",
    "\n",
    "# Перемешайте строки, если необходимо\n",
    "new_df_data_test = new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_1 = data[data['label'] == 1]\n",
    "\n",
    "# Выберите 10% строк, где label=0\n",
    "df_label_0 = data[data['label'] == 0].sample(frac=0.05)\n",
    "\n",
    "# Объедините два DataFrame\n",
    "new_df = pd.concat([df_label_1, df_label_0])\n",
    "\n",
    "# Перемешайте строки, если необходимо\n",
    "new_df_data = new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_train = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MobileBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type mobilebert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of MobileBertModel were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.embedding_transformation.bias', 'embeddings.embedding_transformation.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.bottleneck.attention.LayerNorm.bias', 'encoder.layer.0.bottleneck.attention.LayerNorm.weight', 'encoder.layer.0.bottleneck.attention.dense.bias', 'encoder.layer.0.bottleneck.attention.dense.weight', 'encoder.layer.0.bottleneck.input.LayerNorm.bias', 'encoder.layer.0.bottleneck.input.LayerNorm.weight', 'encoder.layer.0.bottleneck.input.dense.bias', 'encoder.layer.0.bottleneck.input.dense.weight', 'encoder.layer.0.ffn.0.intermediate.dense.bias', 'encoder.layer.0.ffn.0.intermediate.dense.weight', 'encoder.layer.0.ffn.0.output.LayerNorm.bias', 'encoder.layer.0.ffn.0.output.LayerNorm.weight', 'encoder.layer.0.ffn.0.output.dense.bias', 'encoder.layer.0.ffn.0.output.dense.weight', 'encoder.layer.0.ffn.1.intermediate.dense.bias', 'encoder.layer.0.ffn.1.intermediate.dense.weight', 'encoder.layer.0.ffn.1.output.LayerNorm.bias', 'encoder.layer.0.ffn.1.output.LayerNorm.weight', 'encoder.layer.0.ffn.1.output.dense.bias', 'encoder.layer.0.ffn.1.output.dense.weight', 'encoder.layer.0.ffn.2.intermediate.dense.bias', 'encoder.layer.0.ffn.2.intermediate.dense.weight', 'encoder.layer.0.ffn.2.output.LayerNorm.bias', 'encoder.layer.0.ffn.2.output.LayerNorm.weight', 'encoder.layer.0.ffn.2.output.dense.bias', 'encoder.layer.0.ffn.2.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.bottleneck.LayerNorm.bias', 'encoder.layer.0.output.bottleneck.LayerNorm.weight', 'encoder.layer.0.output.bottleneck.dense.bias', 'encoder.layer.0.output.bottleneck.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.bottleneck.attention.LayerNorm.bias', 'encoder.layer.1.bottleneck.attention.LayerNorm.weight', 'encoder.layer.1.bottleneck.attention.dense.bias', 'encoder.layer.1.bottleneck.attention.dense.weight', 'encoder.layer.1.bottleneck.input.LayerNorm.bias', 'encoder.layer.1.bottleneck.input.LayerNorm.weight', 'encoder.layer.1.bottleneck.input.dense.bias', 'encoder.layer.1.bottleneck.input.dense.weight', 'encoder.layer.1.ffn.0.intermediate.dense.bias', 'encoder.layer.1.ffn.0.intermediate.dense.weight', 'encoder.layer.1.ffn.0.output.LayerNorm.bias', 'encoder.layer.1.ffn.0.output.LayerNorm.weight', 'encoder.layer.1.ffn.0.output.dense.bias', 'encoder.layer.1.ffn.0.output.dense.weight', 'encoder.layer.1.ffn.1.intermediate.dense.bias', 'encoder.layer.1.ffn.1.intermediate.dense.weight', 'encoder.layer.1.ffn.1.output.LayerNorm.bias', 'encoder.layer.1.ffn.1.output.LayerNorm.weight', 'encoder.layer.1.ffn.1.output.dense.bias', 'encoder.layer.1.ffn.1.output.dense.weight', 'encoder.layer.1.ffn.2.intermediate.dense.bias', 'encoder.layer.1.ffn.2.intermediate.dense.weight', 'encoder.layer.1.ffn.2.output.LayerNorm.bias', 'encoder.layer.1.ffn.2.output.LayerNorm.weight', 'encoder.layer.1.ffn.2.output.dense.bias', 'encoder.layer.1.ffn.2.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.bottleneck.LayerNorm.bias', 'encoder.layer.1.output.bottleneck.LayerNorm.weight', 'encoder.layer.1.output.bottleneck.dense.bias', 'encoder.layer.1.output.bottleneck.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "token = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "model = MobileBertModel.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,#128\n",
    "            truncation=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wrapper = TokenizerWrapper(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68272f16615c41d09313a383cec4eee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/4457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "    tokenizer_wrapper.tokenize_function, \n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63e5d8152fb455583987a701e310a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1115 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = dataset_test.map(\n",
    "    tokenizer_wrapper.tokenize_function, \n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_proc=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/bert_uncased_L-2_H-128_A-2\",#\"google/mobilebert-uncased\",  \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "    num_labels=2, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,386,178 total parameters.\n",
      "4,386,178 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_uncased_L-2_H-128_A-2\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    #fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class_weights=class_weights.to(\"cuda\")\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=token,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef4cdac1914459891e090162ccccead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e1645cf544c56bc2f69a15bdaa6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09160401672124863, 'eval_accuracy': 0.9883408071748879, 'eval_runtime': 0.6833, 'eval_samples_per_second': 1631.895, 'eval_steps_per_second': 102.451, 'epoch': 1.0}\n",
      "{'loss': 0.1278, 'grad_norm': 23.255704879760742, 'learning_rate': 0.00016415770609318997, 'epoch': 1.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5658f582eda04a23aec21cc6977a6306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09056468307971954, 'eval_accuracy': 0.9910313901345291, 'eval_runtime': 0.6736, 'eval_samples_per_second': 1655.191, 'eval_steps_per_second': 103.913, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5a11fb69974f0ba19c2091e95d5032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12090333551168442, 'eval_accuracy': 0.9901345291479821, 'eval_runtime': 0.6338, 'eval_samples_per_second': 1759.147, 'eval_steps_per_second': 110.44, 'epoch': 3.0}\n",
      "{'loss': 0.0298, 'grad_norm': 0.011278289370238781, 'learning_rate': 0.0001283154121863799, 'epoch': 3.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1d808cbbf24236b080efb8d5bee9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1180289089679718, 'eval_accuracy': 0.9901345291479821, 'eval_runtime': 0.6171, 'eval_samples_per_second': 1806.709, 'eval_steps_per_second': 113.426, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77b975e590642efbcf722ac08626c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12161996960639954, 'eval_accuracy': 0.989237668161435, 'eval_runtime': 0.6332, 'eval_samples_per_second': 1761.003, 'eval_steps_per_second': 110.556, 'epoch': 5.0}\n",
      "{'loss': 0.002, 'grad_norm': 0.004528995603322983, 'learning_rate': 9.247311827956989e-05, 'epoch': 5.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d03afca75746b1aecfd93d1a106565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11960107088088989, 'eval_accuracy': 0.989237668161435, 'eval_runtime': 0.6154, 'eval_samples_per_second': 1811.71, 'eval_steps_per_second': 113.74, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bcbbb553814ecfaf6c2aab45984cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14873838424682617, 'eval_accuracy': 0.9919282511210762, 'eval_runtime': 0.6387, 'eval_samples_per_second': 1745.851, 'eval_steps_per_second': 109.605, 'epoch': 7.0}\n",
      "{'loss': 0.0024, 'grad_norm': 0.0033436098601669073, 'learning_rate': 5.663082437275986e-05, 'epoch': 7.17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06ef8131617489dad0836056ee0d048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14282935857772827, 'eval_accuracy': 0.9901345291479821, 'eval_runtime': 0.6537, 'eval_samples_per_second': 1705.645, 'eval_steps_per_second': 107.081, 'epoch': 8.0}\n",
      "{'loss': 0.0002, 'grad_norm': 0.002743934979662299, 'learning_rate': 2.078853046594982e-05, 'epoch': 8.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc291dd465743c4bb2f32c358b7f6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14531202614307404, 'eval_accuracy': 0.9910313901345291, 'eval_runtime': 0.6002, 'eval_samples_per_second': 1857.855, 'eval_steps_per_second': 116.637, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266ce918b9fb4a09929e4cc767d6c758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14768941700458527, 'eval_accuracy': 0.9901345291479821, 'eval_runtime': 0.6072, 'eval_samples_per_second': 1836.409, 'eval_steps_per_second': 115.29, 'epoch': 10.0}\n",
      "{'train_runtime': 55.0734, 'train_samples_per_second': 809.283, 'train_steps_per_second': 50.66, 'train_loss': 0.029093978527305803, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7236a19b4642bb9d1be0da14c19443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Получаем предсказания\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "\n",
    "# # Предсказанные классы\n",
    "predicted_classes = predictions.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.980633\n",
      "Recall: 0.980633\n",
      "F1-Score: 0.980633\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Precision: %f\" % metrics.precision_score(data_test[\"label\"], predicted_classes, average='macro'))\n",
    "print(\"Recall: %f\" % metrics.recall_score(data_test[\"label\"], predicted_classes, average='macro'))\n",
    "print(\"F1-Score: %f\" % metrics.f1_score(data_test[\"label\"], predicted_classes, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[961,   5],\n",
       "       [  5, 144]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "confusion_matrix(data_test[\"label\"], predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "accelerator.save_model(model, \"./out\", max_shard_size=\"1GB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"optimum-cli\" �� ���� ����७��� ��� ���譥�\n",
      "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model \"./bert_uncased_L-2_H-128_A-2/checkpoint-37500\"  --task text-classification squad_onnx_128/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
