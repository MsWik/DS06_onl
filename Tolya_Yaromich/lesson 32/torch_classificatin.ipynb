{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Сверточные нейронные сети\n",
    "Реализация стеккинга обучения моделей и метамадели на датасете из игральных карт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1660 Ti with Max-Q Design'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "0\n",
      "torch.Size([64, 3, 64, 64])\n",
      "1\n",
      "torch.Size([64, 3, 64, 64])\n",
      "2\n",
      "torch.Size([64, 3, 64, 64])\n",
      "3\n",
      "torch.Size([64, 3, 64, 64])\n",
      "4\n",
      "torch.Size([64, 3, 64, 64])\n",
      "5\n",
      "torch.Size([64, 3, 64, 64])\n",
      "6\n",
      "torch.Size([64, 3, 64, 64])\n",
      "7\n",
      "torch.Size([64, 3, 64, 64])\n",
      "8\n",
      "torch.Size([64, 3, 64, 64])\n",
      "9\n",
      "torch.Size([64, 3, 64, 64])\n",
      "10\n",
      "torch.Size([64, 3, 64, 64])\n",
      "11\n",
      "torch.Size([64, 3, 64, 64])\n",
      "12\n",
      "torch.Size([64, 3, 64, 64])\n",
      "13\n",
      "torch.Size([64, 3, 64, 64])\n",
      "14\n",
      "torch.Size([64, 3, 64, 64])\n",
      "15\n",
      "torch.Size([64, 3, 64, 64])\n",
      "16\n",
      "torch.Size([64, 3, 64, 64])\n",
      "17\n",
      "torch.Size([64, 3, 64, 64])\n",
      "18\n",
      "torch.Size([64, 3, 64, 64])\n",
      "19\n",
      "torch.Size([64, 3, 64, 64])\n",
      "20\n",
      "torch.Size([64, 3, 64, 64])\n",
      "21\n",
      "torch.Size([64, 3, 64, 64])\n",
      "22\n",
      "torch.Size([64, 3, 64, 64])\n",
      "23\n",
      "torch.Size([64, 3, 64, 64])\n",
      "24\n",
      "torch.Size([64, 3, 64, 64])\n",
      "25\n",
      "torch.Size([64, 3, 64, 64])\n",
      "26\n",
      "torch.Size([64, 3, 64, 64])\n",
      "27\n",
      "torch.Size([64, 3, 64, 64])\n",
      "28\n",
      "torch.Size([64, 3, 64, 64])\n",
      "29\n",
      "torch.Size([64, 3, 64, 64])\n",
      "30\n",
      "torch.Size([64, 3, 64, 64])\n",
      "31\n",
      "torch.Size([64, 3, 64, 64])\n",
      "32\n",
      "torch.Size([64, 3, 64, 64])\n",
      "33\n",
      "torch.Size([64, 3, 64, 64])\n",
      "34\n",
      "torch.Size([64, 3, 64, 64])\n",
      "35\n",
      "torch.Size([64, 3, 64, 64])\n",
      "36\n",
      "torch.Size([64, 3, 64, 64])\n",
      "37\n",
      "torch.Size([64, 3, 64, 64])\n",
      "38\n",
      "torch.Size([64, 3, 64, 64])\n",
      "39\n",
      "torch.Size([64, 3, 64, 64])\n",
      "40\n",
      "torch.Size([64, 3, 64, 64])\n",
      "41\n",
      "torch.Size([64, 3, 64, 64])\n",
      "42\n",
      "torch.Size([64, 3, 64, 64])\n",
      "43\n",
      "torch.Size([64, 3, 64, 64])\n",
      "44\n",
      "torch.Size([64, 3, 64, 64])\n",
      "45\n",
      "torch.Size([64, 3, 64, 64])\n",
      "46\n",
      "torch.Size([64, 3, 64, 64])\n",
      "47\n",
      "torch.Size([64, 3, 64, 64])\n",
      "48\n",
      "torch.Size([64, 3, 64, 64])\n",
      "49\n",
      "torch.Size([64, 3, 64, 64])\n",
      "50\n",
      "torch.Size([64, 3, 64, 64])\n",
      "51\n",
      "torch.Size([64, 3, 64, 64])\n",
      "52\n",
      "torch.Size([64, 3, 64, 64])\n",
      "53\n",
      "torch.Size([64, 3, 64, 64])\n",
      "54\n",
      "torch.Size([64, 3, 64, 64])\n",
      "55\n",
      "torch.Size([64, 3, 64, 64])\n",
      "56\n",
      "torch.Size([64, 3, 64, 64])\n",
      "57\n",
      "torch.Size([64, 3, 64, 64])\n",
      "58\n",
      "torch.Size([64, 3, 64, 64])\n",
      "59\n",
      "torch.Size([64, 3, 64, 64])\n",
      "60\n",
      "torch.Size([64, 3, 64, 64])\n",
      "61\n",
      "torch.Size([64, 3, 64, 64])\n",
      "62\n",
      "torch.Size([64, 3, 64, 64])\n",
      "63\n",
      "torch.Size([64, 3, 64, 64])\n",
      "64\n",
      "torch.Size([64, 3, 64, 64])\n",
      "65\n",
      "torch.Size([64, 3, 64, 64])\n",
      "66\n",
      "torch.Size([64, 3, 64, 64])\n",
      "67\n",
      "torch.Size([64, 3, 64, 64])\n",
      "68\n",
      "torch.Size([64, 3, 64, 64])\n",
      "69\n",
      "torch.Size([64, 3, 64, 64])\n",
      "70\n",
      "torch.Size([64, 3, 64, 64])\n",
      "71\n",
      "torch.Size([64, 3, 64, 64])\n",
      "72\n",
      "torch.Size([64, 3, 64, 64])\n",
      "73\n",
      "torch.Size([64, 3, 64, 64])\n",
      "74\n",
      "torch.Size([64, 3, 64, 64])\n",
      "75\n",
      "torch.Size([64, 3, 64, 64])\n",
      "76\n",
      "torch.Size([64, 3, 64, 64])\n",
      "77\n",
      "torch.Size([64, 3, 64, 64])\n",
      "78\n",
      "torch.Size([64, 3, 64, 64])\n",
      "79\n",
      "torch.Size([64, 3, 64, 64])\n",
      "80\n",
      "torch.Size([64, 3, 64, 64])\n",
      "81\n",
      "torch.Size([64, 3, 64, 64])\n",
      "82\n",
      "torch.Size([64, 3, 64, 64])\n",
      "83\n",
      "torch.Size([64, 3, 64, 64])\n",
      "84\n",
      "torch.Size([64, 3, 64, 64])\n",
      "85\n",
      "torch.Size([64, 3, 64, 64])\n",
      "86\n",
      "torch.Size([64, 3, 64, 64])\n",
      "87\n",
      "torch.Size([64, 3, 64, 64])\n",
      "88\n",
      "torch.Size([64, 3, 64, 64])\n",
      "89\n",
      "torch.Size([64, 3, 64, 64])\n",
      "90\n",
      "torch.Size([64, 3, 64, 64])\n",
      "91\n",
      "torch.Size([64, 3, 64, 64])\n",
      "92\n",
      "torch.Size([64, 3, 64, 64])\n",
      "93\n",
      "torch.Size([64, 3, 64, 64])\n",
      "94\n",
      "torch.Size([64, 3, 64, 64])\n",
      "95\n",
      "torch.Size([64, 3, 64, 64])\n",
      "96\n",
      "torch.Size([64, 3, 64, 64])\n",
      "97\n",
      "torch.Size([64, 3, 64, 64])\n",
      "98\n",
      "torch.Size([64, 3, 64, 64])\n",
      "99\n",
      "torch.Size([64, 3, 64, 64])\n",
      "100\n",
      "torch.Size([64, 3, 64, 64])\n",
      "101\n",
      "torch.Size([64, 3, 64, 64])\n",
      "102\n",
      "torch.Size([64, 3, 64, 64])\n",
      "103\n",
      "torch.Size([64, 3, 64, 64])\n",
      "104\n",
      "torch.Size([64, 3, 64, 64])\n",
      "105\n",
      "torch.Size([64, 3, 64, 64])\n",
      "106\n",
      "torch.Size([64, 3, 64, 64])\n",
      "107\n",
      "torch.Size([64, 3, 64, 64])\n",
      "108\n",
      "torch.Size([64, 3, 64, 64])\n",
      "109\n",
      "torch.Size([64, 3, 64, 64])\n",
      "110\n",
      "torch.Size([64, 3, 64, 64])\n",
      "111\n",
      "torch.Size([64, 3, 64, 64])\n",
      "112\n",
      "torch.Size([64, 3, 64, 64])\n",
      "113\n",
      "torch.Size([64, 3, 64, 64])\n",
      "114\n",
      "torch.Size([64, 3, 64, 64])\n",
      "115\n",
      "torch.Size([64, 3, 64, 64])\n",
      "116\n",
      "torch.Size([64, 3, 64, 64])\n",
      "117\n",
      "torch.Size([64, 3, 64, 64])\n",
      "118\n",
      "torch.Size([8, 3, 64, 64])\n",
      "119\n"
     ]
    }
   ],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "train_data_path = \"./train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "val_data_path = \"./valid/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "test_data_path = \"./test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms, is_valid_file=check_image) \n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size) \n",
    "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size) \n",
    "\n",
    "for batch_ndx, sample in enumerate(train_data_loader):\n",
    "    print(sample[0].shape)\n",
    "    print(batch_ndx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_test =\"./cards.csv\"\n",
    "data = pd.read_csv(file_path_test, encoding='cp1251', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"labels\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=53):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNNNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cnnnet \u001b[38;5;241m=\u001b[39m \u001b[43mCNNNet\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CNNNet' is not defined"
     ]
    }
   ],
   "source": [
    "cnnnet = CNNNet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnnet(torch.rand(1, 3,224, 224, requires_grad=True))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cuda:0\"):\n",
    "    acc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1],\n",
    "                               targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        if (acc< num_correct / num_examples):\n",
    "                acc = num_correct / num_examples\n",
    "                \n",
    "        if (19< epoch):\n",
    "                acc = num_correct / num_examples\n",
    "                torch.save(model,f'model{epoch}_{acc}.pt')        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_data_path = \"./train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "val_data_path = \"./valid/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "batch_size=32\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "device = torch.device(\"cuda\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnnet.to(device)\n",
    "optimizer = optim.Adam(cnnnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 3.55, Validation Loss: 2.55, accuracy = 0.23\n",
      "Epoch: 2, Training Loss: 2.47, Validation Loss: 1.89, accuracy = 0.33\n",
      "Epoch: 3, Training Loss: 2.14, Validation Loss: 1.72, accuracy = 0.39\n",
      "Epoch: 4, Training Loss: 1.95, Validation Loss: 1.40, accuracy = 0.55\n",
      "Epoch: 5, Training Loss: 1.78, Validation Loss: 1.37, accuracy = 0.53\n",
      "Epoch: 6, Training Loss: 1.67, Validation Loss: 1.29, accuracy = 0.55\n",
      "Epoch: 7, Training Loss: 1.62, Validation Loss: 1.34, accuracy = 0.57\n",
      "Epoch: 8, Training Loss: 1.50, Validation Loss: 1.12, accuracy = 0.66\n",
      "Epoch: 9, Training Loss: 1.35, Validation Loss: 1.01, accuracy = 0.74\n",
      "Epoch: 10, Training Loss: 1.27, Validation Loss: 0.92, accuracy = 0.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnnnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m     training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     18\u001b[0m training_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(cnnnet, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=21, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anon/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "# Загрузка предварительно обученной модели MobileNet\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "# Перемещение модели на устройство (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Пример загрузки изображения и его преобразования в тензор\n",
    "input_image = Image.open(\"./train/ace of clubs/001.jpg\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Применение преобразований к изображению\n",
    "input_tensor = preprocess(input_image)\n",
    "\n",
    "# Добавление размерности батча (batch dimension) для ожидаемого входа модели\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "# Передача тензора на устройство (GPU или CPU)\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Изменение последнего слоя классификатора модели для адаптации к вашим данным\n",
    "num_classes = 53\n",
    "model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anon/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Определение функции обучения\n",
    "# Определение функции обучения\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cuda:0\"):\n",
    "    acc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Перемещаем модель на устройство\n",
    "            model.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            # Перемещаем модель на устройство\n",
    "            model.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output, targets) \n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        if (acc< num_correct / num_examples):\n",
    "            acc = num_correct / num_examples\n",
    "                \n",
    "        if (19< epoch):\n",
    "            acc = num_correct / num_examples\n",
    "            torch.save(model, f'model{epoch}_{acc}.pt')        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "batch_size = 16\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "# Функция проверки изображения\n",
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Трансформации для изображений\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Пути к данным\n",
    "train_data_path = \"./train/\"\n",
    "val_data_path = \"./valid/\"\n",
    "\n",
    "# Создание датасетов\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=img_transforms, is_valid_file=check_image)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=img_transforms, is_valid_file=check_image)\n",
    "\n",
    "# DataLoader для обучения и валидации\n",
    "batch_size = 10\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка предварительно обученной модели MobileNet\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "# Перемещение модели на устройство (GPU или CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Изменение последнего слоя классификатора модели для адаптации к вашим данным\n",
    "num_classes = 53\n",
    "model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anon\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_0-b66bff10.pth\" to C:\\Users\\anon/.cache\\torch\\hub\\checkpoints\\squeezenet1_0-b66bff10.pth\n",
      "100%|██████████| 4.78M/4.78M [00:18<00:00, 265kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Определение функции обучения\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cuda:0\"):\n",
    "    acc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Перемещаем модель на устройство\n",
    "            model.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            # Перемещаем модель на устройство\n",
    "            model.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output, targets) \n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        if (acc< num_correct / num_examples):\n",
    "            acc = num_correct / num_examples\n",
    "                \n",
    "        if (19< epoch):\n",
    "            acc = num_correct / num_examples\n",
    "            torch.save(model, f'model{epoch}_{acc}.pt')        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))\n",
    "\n",
    "# Функция проверки изображения\n",
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Трансформации для изображений\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Пути к данным\n",
    "train_data_path = \"./train/\"\n",
    "val_data_path = \"./valid/\"\n",
    "\n",
    "# Создание датасетов\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=img_transforms, is_valid_file=check_image)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=img_transforms, is_valid_file=check_image)\n",
    "\n",
    "# DataLoader для обучения и валидации\n",
    "batch_size = 10\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка предварительно обученной модели SqueezeNet\n",
    "model = torchvision.models.squeezenet1_0(pretrained=True)\n",
    "\n",
    "# Перемещение модели на устройство (GPU или CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Изменение последнего слоя классификатора модели для адаптации к вашим данным\n",
    "num_classes = 53  # ваше количество классов\n",
    "model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss_fn\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 3.97, Validation Loss: 3.94, accuracy = 0.03\n",
      "Epoch: 2, Training Loss: 3.87, Validation Loss: 3.88, accuracy = 0.02\n",
      "Epoch: 3, Training Loss: 3.36, Validation Loss: 2.85, accuracy = 0.18\n",
      "Epoch: 4, Training Loss: 2.68, Validation Loss: 2.04, accuracy = 0.37\n",
      "Epoch: 5, Training Loss: 2.21, Validation Loss: 1.64, accuracy = 0.49\n",
      "Epoch: 6, Training Loss: 1.89, Validation Loss: 1.47, accuracy = 0.54\n",
      "Epoch: 7, Training Loss: 1.66, Validation Loss: 1.23, accuracy = 0.62\n",
      "Epoch: 8, Training Loss: 1.49, Validation Loss: 1.02, accuracy = 0.66\n",
      "Epoch: 9, Training Loss: 1.36, Validation Loss: 0.97, accuracy = 0.70\n",
      "Epoch: 10, Training Loss: 1.23, Validation Loss: 0.88, accuracy = 0.72\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_data_loader, val_data_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#model.half()\n",
    "x = torch.rand(1, 3,224, 224, requires_grad=True).to(device)\n",
    "torch_out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели в формате ONNX\n",
    "torch.onnx.export(model,                   # Модель\n",
    "                  torch.randn(1, 3, 224, 224).to(device), # Пример входных данных\n",
    "                  \"squeezenet.onnx\",            # Путь для сохранения\n",
    "                  export_params=True,       # Сохранять ли параметры модели\n",
    "                  opset_version=11,         # Версия ONNX\n",
    "                  do_constant_folding=True, # Оптимизировать ли константные узлы\n",
    "                  input_names=['input'],    # Имена входных узлов\n",
    "                  output_names=['output'],  # Имена выходных узлов\n",
    "                  dynamic_axes={'input': {0: 'batch_size'}, # Динамические оси\n",
    "                                'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"./squeezenet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_image(model_path, image_path, img_transforms, device):\n",
    "    # Загрузка ONNX модели\n",
    "    session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "    # Открытие изображения\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Применение преобразований к изображению\n",
    "    image = img_transforms(image).unsqueeze(0)\n",
    "\n",
    "    # Получение входного имени и выходного имени из модели\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    # Выполнение предсказания\n",
    "    input_data = image.cpu().numpy() if device == torch.device(\"cuda\") else image.numpy()\n",
    "\n",
    "    outputs = session.run([output_name], {input_name: input_data})\n",
    "\n",
    "    # Получение индекса предсказанного класса\n",
    "    predicted_class = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "из перебранных мной ace of clubs он угадал 4 из 5,интересно что он везде угдал масть но в перой картинке предположи что это король вместо туза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предсказанный класс: 0\n",
      "Имя класса : ace of clubs\n"
     ]
    }
   ],
   "source": [
    "predicted_class = predict_image(\"./squeezenet.onnx\", \"./test/ace of clubs/5.jpg\", img_transforms, device)\n",
    "print(\"Предсказанный класс:\", predicted_class)\n",
    "\n",
    "class_names = train_data.classes\n",
    "print(\"Имя класса :\",class_names[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import onnxruntime\n",
    "\n",
    "# Функция для предсказания класса изображения\n",
    "def predict_image(model_path, image_path, img_transforms, device):\n",
    "    # Загрузка ONNX модели\n",
    "    session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "    # Открытие изображения\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Применение преобразований к изображению\n",
    "    image = img_transforms(image).unsqueeze(0)\n",
    "\n",
    "    # Получение входного имени и выходного имени из модели\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    # Выполнение предсказания\n",
    "    input_data = image.cpu().numpy() if device == torch.device(\"cuda\") else image.numpy()\n",
    "    outputs = session.run([output_name], {input_name: input_data})\n",
    "\n",
    "    # Получение индекса предсказанного класса\n",
    "    predicted_class = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n",
    "# Путь к вашей модели ONNX\n",
    "model_path = \"mobile.onnx\"\n",
    "\n",
    "# Путь к вашему тестовому набору данных\n",
    "test_data_path = \"./test/\"\n",
    "\n",
    "\n",
    "# Преобразования изображений\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Инициализация списка для хранения предсказанных классов и истинных меток\n",
    "predicted_classes = []\n",
    "true_labels = []\n",
    "\n",
    "# Пройти по всем изображениям в тестовом наборе данных\n",
    "for root, dirs, files in os.walk(test_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\"):  # Проверка, что файл - изображение\n",
    "            image_path = os.path.join(root, file)\n",
    "            true_label = os.path.basename(root)  # Извлечение истинного класса из пути\n",
    "            predicted_class = predict_image(model_path, image_path, img_transforms, device)\n",
    "            predicted_classes.append(predicted_class)\n",
    "            true_labels.append(true_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "untocken_classes = []\n",
    "for i in predicted_classes:\n",
    "    untocken_classes.append(class_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclass_names\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Вычисление метрик\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(true_labels, untocken_classes)\n\u001b[0;32m      3\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, untocken_classes, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, untocken_classes, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Вычисление метрик\n",
    "accuracy = accuracy_score(true_labels, untocken_classes)\n",
    "precision = precision_score(true_labels, untocken_classes, average='weighted')\n",
    "recall = recall_score(true_labels, untocken_classes, average='weighted')\n",
    "f1 = f1_score(true_labels, untocken_classes, average='weighted')\n",
    "\n",
    "# Вывод метрик\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.6830188679245283\n",
    "Precision: 0.749387771557583\n",
    "Recall: 0.6830188679245283\n",
    "F1 Score: 0.677947628891025 это метрики mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import onnxruntime\n",
    "\n",
    "# Функция для предсказания класса изображения для двух моделей\n",
    "def predict_image(model_paths, image_path, img_transforms, device):\n",
    "    predicted_classes = []\n",
    "    for model_path in model_paths:\n",
    "        # Загрузка ONNX модели\n",
    "        session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "        # Открытие изображения\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Применение преобразований к изображению\n",
    "        image = img_transforms(image).unsqueeze(0)\n",
    "\n",
    "        # Получение входного имени и выходного имени из модели\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_name = session.get_outputs()[0].name\n",
    "\n",
    "        # Выполнение предсказания\n",
    "        input_data = image.cpu().numpy() if device == torch.device(\"cuda\") else image.numpy()\n",
    "        outputs = session.run([output_name], {input_name: input_data})\n",
    "\n",
    "        # Получение индекса предсказанного класса\n",
    "        predicted_class = np.argmax(outputs[0], axis=1)\n",
    "        predicted_classes.append(predicted_class.item())\n",
    "\n",
    "    return predicted_classes\n",
    "\n",
    "# Пути к вашим моделям ONNX\n",
    "model_paths = [\"mobile.onnx\", \"model.onnx\",\"squeezenet.onnx\"]\n",
    "\n",
    "# Пути к вашим данным для предсказания\n",
    "train_data_path = \"./test/\"\n",
    "val_data_path = \"./valid/\"\n",
    "\n",
    "# Преобразования изображений\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Инициализация списка для хранения предсказанных классов и истинных меток\n",
    "predicted_classes_model1 = []\n",
    "predicted_classes_model2 = []\n",
    "predicted_classes_model3 = []\n",
    "true_labels = []\n",
    "\n",
    "# Пройти по всем изображениям в тренировочном и валидационном наборах данных\n",
    "for data_path in [train_data_path, val_data_path]:\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):  # Проверка, что файл - изображение\n",
    "                image_path = os.path.join(root, file)\n",
    "                true_label = os.path.basename(root)  # Извлечение истинного класса из пути\n",
    "                predicted_classes = predict_image(model_paths, image_path, img_transforms, device)\n",
    "                predicted_classes_model1.append(predicted_classes[0])\n",
    "                predicted_classes_model2.append(predicted_classes[1])\n",
    "                predicted_classes_model3.append(predicted_classes[1])\n",
    "                true_labels.append(true_label)\n",
    "\n",
    "# Создание датасета\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "    'Model1_Predictions': predicted_classes_model1,\n",
    "    'Model2_Predictions': predicted_classes_model2,\n",
    "    'Model3_Predictions': predicted_classes_model3,\n",
    "    'True_Labels': true_labels\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model1_Predictions</th>\n",
       "      <th>Model2_Predictions</th>\n",
       "      <th>Model3_Predictions</th>\n",
       "      <th>True_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>ace of clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ace of clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ace of clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ace of clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ace of clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>two of spades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>two of spades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>two of spades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>two of spades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>two of spades</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model1_Predictions  Model2_Predictions  Model3_Predictions    True_Labels\n",
       "0                    36                  21                  21   ace of clubs\n",
       "1                     0                   0                   0   ace of clubs\n",
       "2                     3                   0                   0   ace of clubs\n",
       "3                     0                   0                   0   ace of clubs\n",
       "4                     3                   0                   0   ace of clubs\n",
       "..                  ...                 ...                 ...            ...\n",
       "525                  52                  52                  52  two of spades\n",
       "526                  45                  32                  32  two of spades\n",
       "527                  52                  52                  52  two of spades\n",
       "528                  52                  52                  52  two of spades\n",
       "529                  52                  40                  40  two of spades\n",
       "\n",
       "[530 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обратное преобразование числовых меток в текстовые метки для столбца True_Labels\n",
    "label_indices = [class_names.index(label) for label in dataset['True_Labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['True_Labels'] = label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model1_Predictions</th>\n",
       "      <th>Model2_Predictions</th>\n",
       "      <th>Model3_Predictions</th>\n",
       "      <th>True_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>52</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>530 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model1_Predictions  Model2_Predictions  Model3_Predictions  True_Labels\n",
       "0                    36                  21                  21            0\n",
       "1                     0                   0                   0            0\n",
       "2                     3                   0                   0            0\n",
       "3                     0                   0                   0            0\n",
       "4                     3                   0                   0            0\n",
       "..                  ...                 ...                 ...          ...\n",
       "525                  52                  52                  52           52\n",
       "526                  45                  32                  32           52\n",
       "527                  52                  52                  52           52\n",
       "528                  52                  52                  52           52\n",
       "529                  52                  40                  40           52\n",
       "\n",
       "[530 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Предположим, что df - ваш DataFrame с данными\n",
    "\n",
    "# Разделение на признаки (X) и целевую переменную (y)\n",
    "X = df.drop(columns=['True_Labels'])  # Укажите имя столбца с целевой переменной\n",
    "y = df['True_Labels']\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model1_Predictions</th>\n",
       "      <th>Model2_Predictions</th>\n",
       "      <th>Model3_Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model1_Predictions  Model2_Predictions  Model3_Predictions\n",
       "140                  28                  28                  28\n",
       "398                  26                  26                  26\n",
       "6                     1                   3                   3\n",
       "334                  13                  13                  13\n",
       "322                  11                  11                  11\n",
       "..                  ...                 ...                 ...\n",
       "341                  15                  15                  15\n",
       "431                  33                  33                  33\n",
       "131                   1                  26                  26\n",
       "338                  14                  14                  14\n",
       "514                  49                  49                  49\n",
       "\n",
       "[106 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140    28\n",
       "398    26\n",
       "6       1\n",
       "334    13\n",
       "322    11\n",
       "       ..\n",
       "341    15\n",
       "431    33\n",
       "131    26\n",
       "338    14\n",
       "514    49\n",
       "Name: True_Labels, Length: 106, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model1_Predictions</th>\n",
       "      <th>Model2_Predictions</th>\n",
       "      <th>Model3_Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model1_Predictions  Model2_Predictions  Model3_Predictions\n",
       "137                  27                  27                  27\n",
       "526                  45                  32                  32\n",
       "417                  30                  30                  30\n",
       "517                  50                   1                   1\n",
       "69                   13                  13                  13\n",
       "..                  ...                 ...                 ...\n",
       "71                   14                  14                  14\n",
       "106                  21                  21                  21\n",
       "270                   1                   1                   1\n",
       "435                  34                  34                  34\n",
       "102                  20                   4                   4\n",
       "\n",
       "[424 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137    27\n",
       "526    52\n",
       "417    30\n",
       "517    50\n",
       "69     13\n",
       "Name: True_Labels, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 6.5281\n",
      "Epoch [2/2000], Loss: 6.1575\n",
      "Epoch [3/2000], Loss: 5.8398\n",
      "Epoch [4/2000], Loss: 5.5683\n",
      "Epoch [5/2000], Loss: 5.3368\n",
      "Epoch [6/2000], Loss: 5.1408\n",
      "Epoch [7/2000], Loss: 4.9730\n",
      "Epoch [8/2000], Loss: 4.8296\n",
      "Epoch [9/2000], Loss: 4.7084\n",
      "Epoch [10/2000], Loss: 4.6032\n",
      "Epoch [11/2000], Loss: 4.5163\n",
      "Epoch [12/2000], Loss: 4.4402\n",
      "Epoch [13/2000], Loss: 4.3743\n",
      "Epoch [14/2000], Loss: 4.3164\n",
      "Epoch [15/2000], Loss: 4.2648\n",
      "Epoch [16/2000], Loss: 4.2184\n",
      "Epoch [17/2000], Loss: 4.1765\n",
      "Epoch [18/2000], Loss: 4.1383\n",
      "Epoch [19/2000], Loss: 4.1053\n",
      "Epoch [20/2000], Loss: 4.0768\n",
      "Epoch [21/2000], Loss: 4.0519\n",
      "Epoch [22/2000], Loss: 4.0305\n",
      "Epoch [23/2000], Loss: 4.0115\n",
      "Epoch [24/2000], Loss: 3.9938\n",
      "Epoch [25/2000], Loss: 3.9777\n",
      "Epoch [26/2000], Loss: 3.9629\n",
      "Epoch [27/2000], Loss: 3.9495\n",
      "Epoch [28/2000], Loss: 3.9374\n",
      "Epoch [29/2000], Loss: 3.9266\n",
      "Epoch [30/2000], Loss: 3.9172\n",
      "Epoch [31/2000], Loss: 3.9088\n",
      "Epoch [32/2000], Loss: 3.9012\n",
      "Epoch [33/2000], Loss: 3.8940\n",
      "Epoch [34/2000], Loss: 3.8873\n",
      "Epoch [35/2000], Loss: 3.8817\n",
      "Epoch [36/2000], Loss: 3.8769\n",
      "Epoch [37/2000], Loss: 3.8729\n",
      "Epoch [38/2000], Loss: 3.8691\n",
      "Epoch [39/2000], Loss: 3.8657\n",
      "Epoch [40/2000], Loss: 3.8630\n",
      "Epoch [41/2000], Loss: 3.8597\n",
      "Epoch [42/2000], Loss: 3.8557\n",
      "Epoch [43/2000], Loss: 3.8524\n",
      "Epoch [44/2000], Loss: 3.8492\n",
      "Epoch [45/2000], Loss: 3.8457\n",
      "Epoch [46/2000], Loss: 3.8422\n",
      "Epoch [47/2000], Loss: 3.8384\n",
      "Epoch [48/2000], Loss: 3.8343\n",
      "Epoch [49/2000], Loss: 3.8299\n",
      "Epoch [50/2000], Loss: 3.8252\n",
      "Epoch [51/2000], Loss: 3.8207\n",
      "Epoch [52/2000], Loss: 3.8159\n",
      "Epoch [53/2000], Loss: 3.8113\n",
      "Epoch [54/2000], Loss: 3.8069\n",
      "Epoch [55/2000], Loss: 3.8026\n",
      "Epoch [56/2000], Loss: 3.7982\n",
      "Epoch [57/2000], Loss: 3.7939\n",
      "Epoch [58/2000], Loss: 3.7893\n",
      "Epoch [59/2000], Loss: 3.7849\n",
      "Epoch [60/2000], Loss: 3.7805\n",
      "Epoch [61/2000], Loss: 3.7761\n",
      "Epoch [62/2000], Loss: 3.7716\n",
      "Epoch [63/2000], Loss: 3.7671\n",
      "Epoch [64/2000], Loss: 3.7628\n",
      "Epoch [65/2000], Loss: 3.7586\n",
      "Epoch [66/2000], Loss: 3.7544\n",
      "Epoch [67/2000], Loss: 3.7502\n",
      "Epoch [68/2000], Loss: 3.7460\n",
      "Epoch [69/2000], Loss: 3.7418\n",
      "Epoch [70/2000], Loss: 3.7376\n",
      "Epoch [71/2000], Loss: 3.7335\n",
      "Epoch [72/2000], Loss: 3.7292\n",
      "Epoch [73/2000], Loss: 3.7246\n",
      "Epoch [74/2000], Loss: 3.7200\n",
      "Epoch [75/2000], Loss: 3.7156\n",
      "Epoch [76/2000], Loss: 3.7115\n",
      "Epoch [77/2000], Loss: 3.7075\n",
      "Epoch [78/2000], Loss: 3.7035\n",
      "Epoch [79/2000], Loss: 3.6993\n",
      "Epoch [80/2000], Loss: 3.6951\n",
      "Epoch [81/2000], Loss: 3.6912\n",
      "Epoch [82/2000], Loss: 3.6871\n",
      "Epoch [83/2000], Loss: 3.6831\n",
      "Epoch [84/2000], Loss: 3.6787\n",
      "Epoch [85/2000], Loss: 3.6744\n",
      "Epoch [86/2000], Loss: 3.6700\n",
      "Epoch [87/2000], Loss: 3.6658\n",
      "Epoch [88/2000], Loss: 3.6617\n",
      "Epoch [89/2000], Loss: 3.6573\n",
      "Epoch [90/2000], Loss: 3.6528\n",
      "Epoch [91/2000], Loss: 3.6484\n",
      "Epoch [92/2000], Loss: 3.6440\n",
      "Epoch [93/2000], Loss: 3.6396\n",
      "Epoch [94/2000], Loss: 3.6352\n",
      "Epoch [95/2000], Loss: 3.6310\n",
      "Epoch [96/2000], Loss: 3.6268\n",
      "Epoch [97/2000], Loss: 3.6224\n",
      "Epoch [98/2000], Loss: 3.6180\n",
      "Epoch [99/2000], Loss: 3.6137\n",
      "Epoch [100/2000], Loss: 3.6095\n",
      "Epoch [101/2000], Loss: 3.6053\n",
      "Epoch [102/2000], Loss: 3.6008\n",
      "Epoch [103/2000], Loss: 3.5963\n",
      "Epoch [104/2000], Loss: 3.5918\n",
      "Epoch [105/2000], Loss: 3.5875\n",
      "Epoch [106/2000], Loss: 3.5831\n",
      "Epoch [107/2000], Loss: 3.5786\n",
      "Epoch [108/2000], Loss: 3.5741\n",
      "Epoch [109/2000], Loss: 3.5696\n",
      "Epoch [110/2000], Loss: 3.5651\n",
      "Epoch [111/2000], Loss: 3.5606\n",
      "Epoch [112/2000], Loss: 3.5559\n",
      "Epoch [113/2000], Loss: 3.5512\n",
      "Epoch [114/2000], Loss: 3.5465\n",
      "Epoch [115/2000], Loss: 3.5417\n",
      "Epoch [116/2000], Loss: 3.5368\n",
      "Epoch [117/2000], Loss: 3.5318\n",
      "Epoch [118/2000], Loss: 3.5265\n",
      "Epoch [119/2000], Loss: 3.5208\n",
      "Epoch [120/2000], Loss: 3.5150\n",
      "Epoch [121/2000], Loss: 3.5093\n",
      "Epoch [122/2000], Loss: 3.5036\n",
      "Epoch [123/2000], Loss: 3.4984\n",
      "Epoch [124/2000], Loss: 3.4932\n",
      "Epoch [125/2000], Loss: 3.4876\n",
      "Epoch [126/2000], Loss: 3.4820\n",
      "Epoch [127/2000], Loss: 3.4767\n",
      "Epoch [128/2000], Loss: 3.4714\n",
      "Epoch [129/2000], Loss: 3.4661\n",
      "Epoch [130/2000], Loss: 3.4609\n",
      "Epoch [131/2000], Loss: 3.4555\n",
      "Epoch [132/2000], Loss: 3.4502\n",
      "Epoch [133/2000], Loss: 3.4448\n",
      "Epoch [134/2000], Loss: 3.4395\n",
      "Epoch [135/2000], Loss: 3.4342\n",
      "Epoch [136/2000], Loss: 3.4289\n",
      "Epoch [137/2000], Loss: 3.4234\n",
      "Epoch [138/2000], Loss: 3.4177\n",
      "Epoch [139/2000], Loss: 3.4121\n",
      "Epoch [140/2000], Loss: 3.4065\n",
      "Epoch [141/2000], Loss: 3.4006\n",
      "Epoch [142/2000], Loss: 3.3945\n",
      "Epoch [143/2000], Loss: 3.3892\n",
      "Epoch [144/2000], Loss: 3.3838\n",
      "Epoch [145/2000], Loss: 3.3782\n",
      "Epoch [146/2000], Loss: 3.3725\n",
      "Epoch [147/2000], Loss: 3.3668\n",
      "Epoch [148/2000], Loss: 3.3614\n",
      "Epoch [149/2000], Loss: 3.3560\n",
      "Epoch [150/2000], Loss: 3.3505\n",
      "Epoch [151/2000], Loss: 3.3449\n",
      "Epoch [152/2000], Loss: 3.3397\n",
      "Epoch [153/2000], Loss: 3.3337\n",
      "Epoch [154/2000], Loss: 3.3285\n",
      "Epoch [155/2000], Loss: 3.3228\n",
      "Epoch [156/2000], Loss: 3.3173\n",
      "Epoch [157/2000], Loss: 3.3119\n",
      "Epoch [158/2000], Loss: 3.3062\n",
      "Epoch [159/2000], Loss: 3.3006\n",
      "Epoch [160/2000], Loss: 3.2955\n",
      "Epoch [161/2000], Loss: 3.2898\n",
      "Epoch [162/2000], Loss: 3.2842\n",
      "Epoch [163/2000], Loss: 3.2788\n",
      "Epoch [164/2000], Loss: 3.2734\n",
      "Epoch [165/2000], Loss: 3.2681\n",
      "Epoch [166/2000], Loss: 3.2627\n",
      "Epoch [167/2000], Loss: 3.2571\n",
      "Epoch [168/2000], Loss: 3.2517\n",
      "Epoch [169/2000], Loss: 3.2462\n",
      "Epoch [170/2000], Loss: 3.2408\n",
      "Epoch [171/2000], Loss: 3.2356\n",
      "Epoch [172/2000], Loss: 3.2300\n",
      "Epoch [173/2000], Loss: 3.2248\n",
      "Epoch [174/2000], Loss: 3.2194\n",
      "Epoch [175/2000], Loss: 3.2140\n",
      "Epoch [176/2000], Loss: 3.2087\n",
      "Epoch [177/2000], Loss: 3.2035\n",
      "Epoch [178/2000], Loss: 3.1981\n",
      "Epoch [179/2000], Loss: 3.1927\n",
      "Epoch [180/2000], Loss: 3.1873\n",
      "Epoch [181/2000], Loss: 3.1822\n",
      "Epoch [182/2000], Loss: 3.1771\n",
      "Epoch [183/2000], Loss: 3.1716\n",
      "Epoch [184/2000], Loss: 3.1662\n",
      "Epoch [185/2000], Loss: 3.1612\n",
      "Epoch [186/2000], Loss: 3.1560\n",
      "Epoch [187/2000], Loss: 3.1509\n",
      "Epoch [188/2000], Loss: 3.1457\n",
      "Epoch [189/2000], Loss: 3.1404\n",
      "Epoch [190/2000], Loss: 3.1356\n",
      "Epoch [191/2000], Loss: 3.1302\n",
      "Epoch [192/2000], Loss: 3.1252\n",
      "Epoch [193/2000], Loss: 3.1200\n",
      "Epoch [194/2000], Loss: 3.1151\n",
      "Epoch [195/2000], Loss: 3.1100\n",
      "Epoch [196/2000], Loss: 3.1048\n",
      "Epoch [197/2000], Loss: 3.0999\n",
      "Epoch [198/2000], Loss: 3.0948\n",
      "Epoch [199/2000], Loss: 3.0898\n",
      "Epoch [200/2000], Loss: 3.0849\n",
      "Epoch [201/2000], Loss: 3.0799\n",
      "Epoch [202/2000], Loss: 3.0749\n",
      "Epoch [203/2000], Loss: 3.0700\n",
      "Epoch [204/2000], Loss: 3.0651\n",
      "Epoch [205/2000], Loss: 3.0602\n",
      "Epoch [206/2000], Loss: 3.0554\n",
      "Epoch [207/2000], Loss: 3.0505\n",
      "Epoch [208/2000], Loss: 3.0457\n",
      "Epoch [209/2000], Loss: 3.0408\n",
      "Epoch [210/2000], Loss: 3.0360\n",
      "Epoch [211/2000], Loss: 3.0312\n",
      "Epoch [212/2000], Loss: 3.0265\n",
      "Epoch [213/2000], Loss: 3.0220\n",
      "Epoch [214/2000], Loss: 3.0172\n",
      "Epoch [215/2000], Loss: 3.0126\n",
      "Epoch [216/2000], Loss: 3.0077\n",
      "Epoch [217/2000], Loss: 3.0033\n",
      "Epoch [218/2000], Loss: 2.9984\n",
      "Epoch [219/2000], Loss: 2.9940\n",
      "Epoch [220/2000], Loss: 2.9892\n",
      "Epoch [221/2000], Loss: 2.9846\n",
      "Epoch [222/2000], Loss: 2.9801\n",
      "Epoch [223/2000], Loss: 2.9755\n",
      "Epoch [224/2000], Loss: 2.9710\n",
      "Epoch [225/2000], Loss: 2.9664\n",
      "Epoch [226/2000], Loss: 2.9620\n",
      "Epoch [227/2000], Loss: 2.9576\n",
      "Epoch [228/2000], Loss: 2.9531\n",
      "Epoch [229/2000], Loss: 2.9488\n",
      "Epoch [230/2000], Loss: 2.9443\n",
      "Epoch [231/2000], Loss: 2.9401\n",
      "Epoch [232/2000], Loss: 2.9358\n",
      "Epoch [233/2000], Loss: 2.9313\n",
      "Epoch [234/2000], Loss: 2.9272\n",
      "Epoch [235/2000], Loss: 2.9228\n",
      "Epoch [236/2000], Loss: 2.9186\n",
      "Epoch [237/2000], Loss: 2.9143\n",
      "Epoch [238/2000], Loss: 2.9100\n",
      "Epoch [239/2000], Loss: 2.9058\n",
      "Epoch [240/2000], Loss: 2.9018\n",
      "Epoch [241/2000], Loss: 2.8975\n",
      "Epoch [242/2000], Loss: 2.8933\n",
      "Epoch [243/2000], Loss: 2.8893\n",
      "Epoch [244/2000], Loss: 2.8853\n",
      "Epoch [245/2000], Loss: 2.8810\n",
      "Epoch [246/2000], Loss: 2.8772\n",
      "Epoch [247/2000], Loss: 2.8731\n",
      "Epoch [248/2000], Loss: 2.8688\n",
      "Epoch [249/2000], Loss: 2.8654\n",
      "Epoch [250/2000], Loss: 2.8611\n",
      "Epoch [251/2000], Loss: 2.8569\n",
      "Epoch [252/2000], Loss: 2.8534\n",
      "Epoch [253/2000], Loss: 2.8492\n",
      "Epoch [254/2000], Loss: 2.8452\n",
      "Epoch [255/2000], Loss: 2.8417\n",
      "Epoch [256/2000], Loss: 2.8375\n",
      "Epoch [257/2000], Loss: 2.8339\n",
      "Epoch [258/2000], Loss: 2.8302\n",
      "Epoch [259/2000], Loss: 2.8259\n",
      "Epoch [260/2000], Loss: 2.8225\n",
      "Epoch [261/2000], Loss: 2.8184\n",
      "Epoch [262/2000], Loss: 2.8146\n",
      "Epoch [263/2000], Loss: 2.8109\n",
      "Epoch [264/2000], Loss: 2.8068\n",
      "Epoch [265/2000], Loss: 2.8031\n",
      "Epoch [266/2000], Loss: 2.7993\n",
      "Epoch [267/2000], Loss: 2.7955\n",
      "Epoch [268/2000], Loss: 2.7917\n",
      "Epoch [269/2000], Loss: 2.7881\n",
      "Epoch [270/2000], Loss: 2.7843\n",
      "Epoch [271/2000], Loss: 2.7806\n",
      "Epoch [272/2000], Loss: 2.7769\n",
      "Epoch [273/2000], Loss: 2.7733\n",
      "Epoch [274/2000], Loss: 2.7696\n",
      "Epoch [275/2000], Loss: 2.7659\n",
      "Epoch [276/2000], Loss: 2.7623\n",
      "Epoch [277/2000], Loss: 2.7591\n",
      "Epoch [278/2000], Loss: 2.7551\n",
      "Epoch [279/2000], Loss: 2.7515\n",
      "Epoch [280/2000], Loss: 2.7479\n",
      "Epoch [281/2000], Loss: 2.7443\n",
      "Epoch [282/2000], Loss: 2.7408\n",
      "Epoch [283/2000], Loss: 2.7372\n",
      "Epoch [284/2000], Loss: 2.7341\n",
      "Epoch [285/2000], Loss: 2.7307\n",
      "Epoch [286/2000], Loss: 2.7271\n",
      "Epoch [287/2000], Loss: 2.7232\n",
      "Epoch [288/2000], Loss: 2.7198\n",
      "Epoch [289/2000], Loss: 2.7164\n",
      "Epoch [290/2000], Loss: 2.7130\n",
      "Epoch [291/2000], Loss: 2.7097\n",
      "Epoch [292/2000], Loss: 2.7068\n",
      "Epoch [293/2000], Loss: 2.7037\n",
      "Epoch [294/2000], Loss: 2.6999\n",
      "Epoch [295/2000], Loss: 2.6964\n",
      "Epoch [296/2000], Loss: 2.6929\n",
      "Epoch [297/2000], Loss: 2.6899\n",
      "Epoch [298/2000], Loss: 2.6867\n",
      "Epoch [299/2000], Loss: 2.6831\n",
      "Epoch [300/2000], Loss: 2.6801\n",
      "Epoch [301/2000], Loss: 2.6770\n",
      "Epoch [302/2000], Loss: 2.6735\n",
      "Epoch [303/2000], Loss: 2.6705\n",
      "Epoch [304/2000], Loss: 2.6673\n",
      "Epoch [305/2000], Loss: 2.6640\n",
      "Epoch [306/2000], Loss: 2.6611\n",
      "Epoch [307/2000], Loss: 2.6581\n",
      "Epoch [308/2000], Loss: 2.6548\n",
      "Epoch [309/2000], Loss: 2.6513\n",
      "Epoch [310/2000], Loss: 2.6483\n",
      "Epoch [311/2000], Loss: 2.6450\n",
      "Epoch [312/2000], Loss: 2.6423\n",
      "Epoch [313/2000], Loss: 2.6396\n",
      "Epoch [314/2000], Loss: 2.6363\n",
      "Epoch [315/2000], Loss: 2.6328\n",
      "Epoch [316/2000], Loss: 2.6301\n",
      "Epoch [317/2000], Loss: 2.6285\n",
      "Epoch [318/2000], Loss: 2.6251\n",
      "Epoch [319/2000], Loss: 2.6207\n",
      "Epoch [320/2000], Loss: 2.6177\n",
      "Epoch [321/2000], Loss: 2.6165\n",
      "Epoch [322/2000], Loss: 2.6129\n",
      "Epoch [323/2000], Loss: 2.6083\n",
      "Epoch [324/2000], Loss: 2.6063\n",
      "Epoch [325/2000], Loss: 2.6043\n",
      "Epoch [326/2000], Loss: 2.5998\n",
      "Epoch [327/2000], Loss: 2.5970\n",
      "Epoch [328/2000], Loss: 2.5955\n",
      "Epoch [329/2000], Loss: 2.5916\n",
      "Epoch [330/2000], Loss: 2.5879\n",
      "Epoch [331/2000], Loss: 2.5868\n",
      "Epoch [332/2000], Loss: 2.5832\n",
      "Epoch [333/2000], Loss: 2.5793\n",
      "Epoch [334/2000], Loss: 2.5771\n",
      "Epoch [335/2000], Loss: 2.5754\n",
      "Epoch [336/2000], Loss: 2.5713\n",
      "Epoch [337/2000], Loss: 2.5689\n",
      "Epoch [338/2000], Loss: 2.5671\n",
      "Epoch [339/2000], Loss: 2.5630\n",
      "Epoch [340/2000], Loss: 2.5601\n",
      "Epoch [341/2000], Loss: 2.5576\n",
      "Epoch [342/2000], Loss: 2.5545\n",
      "Epoch [343/2000], Loss: 2.5514\n",
      "Epoch [344/2000], Loss: 2.5491\n",
      "Epoch [345/2000], Loss: 2.5466\n",
      "Epoch [346/2000], Loss: 2.5433\n",
      "Epoch [347/2000], Loss: 2.5403\n",
      "Epoch [348/2000], Loss: 2.5380\n",
      "Epoch [349/2000], Loss: 2.5351\n",
      "Epoch [350/2000], Loss: 2.5322\n",
      "Epoch [351/2000], Loss: 2.5296\n",
      "Epoch [352/2000], Loss: 2.5271\n",
      "Epoch [353/2000], Loss: 2.5241\n",
      "Epoch [354/2000], Loss: 2.5215\n",
      "Epoch [355/2000], Loss: 2.5193\n",
      "Epoch [356/2000], Loss: 2.5164\n",
      "Epoch [357/2000], Loss: 2.5137\n",
      "Epoch [358/2000], Loss: 2.5109\n",
      "Epoch [359/2000], Loss: 2.5082\n",
      "Epoch [360/2000], Loss: 2.5058\n",
      "Epoch [361/2000], Loss: 2.5030\n",
      "Epoch [362/2000], Loss: 2.5008\n",
      "Epoch [363/2000], Loss: 2.4983\n",
      "Epoch [364/2000], Loss: 2.4955\n",
      "Epoch [365/2000], Loss: 2.4931\n",
      "Epoch [366/2000], Loss: 2.4905\n",
      "Epoch [367/2000], Loss: 2.4880\n",
      "Epoch [368/2000], Loss: 2.4855\n",
      "Epoch [369/2000], Loss: 2.4828\n",
      "Epoch [370/2000], Loss: 2.4802\n",
      "Epoch [371/2000], Loss: 2.4781\n",
      "Epoch [372/2000], Loss: 2.4753\n",
      "Epoch [373/2000], Loss: 2.4727\n",
      "Epoch [374/2000], Loss: 2.4704\n",
      "Epoch [375/2000], Loss: 2.4679\n",
      "Epoch [376/2000], Loss: 2.4653\n",
      "Epoch [377/2000], Loss: 2.4633\n",
      "Epoch [378/2000], Loss: 2.4613\n",
      "Epoch [379/2000], Loss: 2.4585\n",
      "Epoch [380/2000], Loss: 2.4554\n",
      "Epoch [381/2000], Loss: 2.4526\n",
      "Epoch [382/2000], Loss: 2.4501\n",
      "Epoch [383/2000], Loss: 2.4476\n",
      "Epoch [384/2000], Loss: 2.4452\n",
      "Epoch [385/2000], Loss: 2.4430\n",
      "Epoch [386/2000], Loss: 2.4406\n",
      "Epoch [387/2000], Loss: 2.4379\n",
      "Epoch [388/2000], Loss: 2.4359\n",
      "Epoch [389/2000], Loss: 2.4335\n",
      "Epoch [390/2000], Loss: 2.4310\n",
      "Epoch [391/2000], Loss: 2.4279\n",
      "Epoch [392/2000], Loss: 2.4249\n",
      "Epoch [393/2000], Loss: 2.4230\n",
      "Epoch [394/2000], Loss: 2.4203\n",
      "Epoch [395/2000], Loss: 2.4181\n",
      "Epoch [396/2000], Loss: 2.4158\n",
      "Epoch [397/2000], Loss: 2.4129\n",
      "Epoch [398/2000], Loss: 2.4107\n",
      "Epoch [399/2000], Loss: 2.4082\n",
      "Epoch [400/2000], Loss: 2.4063\n",
      "Epoch [401/2000], Loss: 2.4047\n",
      "Epoch [402/2000], Loss: 2.4023\n",
      "Epoch [403/2000], Loss: 2.3990\n",
      "Epoch [404/2000], Loss: 2.3964\n",
      "Epoch [405/2000], Loss: 2.3939\n",
      "Epoch [406/2000], Loss: 2.3918\n",
      "Epoch [407/2000], Loss: 2.3899\n",
      "Epoch [408/2000], Loss: 2.3873\n",
      "Epoch [409/2000], Loss: 2.3846\n",
      "Epoch [410/2000], Loss: 2.3820\n",
      "Epoch [411/2000], Loss: 2.3799\n",
      "Epoch [412/2000], Loss: 2.3777\n",
      "Epoch [413/2000], Loss: 2.3756\n",
      "Epoch [414/2000], Loss: 2.3734\n",
      "Epoch [415/2000], Loss: 2.3712\n",
      "Epoch [416/2000], Loss: 2.3683\n",
      "Epoch [417/2000], Loss: 2.3659\n",
      "Epoch [418/2000], Loss: 2.3641\n",
      "Epoch [419/2000], Loss: 2.3624\n",
      "Epoch [420/2000], Loss: 2.3611\n",
      "Epoch [421/2000], Loss: 2.3586\n",
      "Epoch [422/2000], Loss: 2.3557\n",
      "Epoch [423/2000], Loss: 2.3530\n",
      "Epoch [424/2000], Loss: 2.3502\n",
      "Epoch [425/2000], Loss: 2.3484\n",
      "Epoch [426/2000], Loss: 2.3472\n",
      "Epoch [427/2000], Loss: 2.3449\n",
      "Epoch [428/2000], Loss: 2.3419\n",
      "Epoch [429/2000], Loss: 2.3391\n",
      "Epoch [430/2000], Loss: 2.3368\n",
      "Epoch [431/2000], Loss: 2.3348\n",
      "Epoch [432/2000], Loss: 2.3321\n",
      "Epoch [433/2000], Loss: 2.3302\n",
      "Epoch [434/2000], Loss: 2.3283\n",
      "Epoch [435/2000], Loss: 2.3258\n",
      "Epoch [436/2000], Loss: 2.3234\n",
      "Epoch [437/2000], Loss: 2.3218\n",
      "Epoch [438/2000], Loss: 2.3195\n",
      "Epoch [439/2000], Loss: 2.3177\n",
      "Epoch [440/2000], Loss: 2.3160\n",
      "Epoch [441/2000], Loss: 2.3132\n",
      "Epoch [442/2000], Loss: 2.3101\n",
      "Epoch [443/2000], Loss: 2.3086\n",
      "Epoch [444/2000], Loss: 2.3061\n",
      "Epoch [445/2000], Loss: 2.3042\n",
      "Epoch [446/2000], Loss: 2.3021\n",
      "Epoch [447/2000], Loss: 2.2997\n",
      "Epoch [448/2000], Loss: 2.2973\n",
      "Epoch [449/2000], Loss: 2.2953\n",
      "Epoch [450/2000], Loss: 2.2937\n",
      "Epoch [451/2000], Loss: 2.2927\n",
      "Epoch [452/2000], Loss: 2.2907\n",
      "Epoch [453/2000], Loss: 2.2873\n",
      "Epoch [454/2000], Loss: 2.2850\n",
      "Epoch [455/2000], Loss: 2.2837\n",
      "Epoch [456/2000], Loss: 2.2830\n",
      "Epoch [457/2000], Loss: 2.2813\n",
      "Epoch [458/2000], Loss: 2.2774\n",
      "Epoch [459/2000], Loss: 2.2747\n",
      "Epoch [460/2000], Loss: 2.2735\n",
      "Epoch [461/2000], Loss: 2.2731\n",
      "Epoch [462/2000], Loss: 2.2698\n",
      "Epoch [463/2000], Loss: 2.2664\n",
      "Epoch [464/2000], Loss: 2.2652\n",
      "Epoch [465/2000], Loss: 2.2638\n",
      "Epoch [466/2000], Loss: 2.2615\n",
      "Epoch [467/2000], Loss: 2.2586\n",
      "Epoch [468/2000], Loss: 2.2566\n",
      "Epoch [469/2000], Loss: 2.2549\n",
      "Epoch [470/2000], Loss: 2.2535\n",
      "Epoch [471/2000], Loss: 2.2508\n",
      "Epoch [472/2000], Loss: 2.2487\n",
      "Epoch [473/2000], Loss: 2.2472\n",
      "Epoch [474/2000], Loss: 2.2457\n",
      "Epoch [475/2000], Loss: 2.2444\n",
      "Epoch [476/2000], Loss: 2.2416\n",
      "Epoch [477/2000], Loss: 2.2393\n",
      "Epoch [478/2000], Loss: 2.2372\n",
      "Epoch [479/2000], Loss: 2.2351\n",
      "Epoch [480/2000], Loss: 2.2334\n",
      "Epoch [481/2000], Loss: 2.2316\n",
      "Epoch [482/2000], Loss: 2.2299\n",
      "Epoch [483/2000], Loss: 2.2280\n",
      "Epoch [484/2000], Loss: 2.2255\n",
      "Epoch [485/2000], Loss: 2.2234\n",
      "Epoch [486/2000], Loss: 2.2230\n",
      "Epoch [487/2000], Loss: 2.2216\n",
      "Epoch [488/2000], Loss: 2.2192\n",
      "Epoch [489/2000], Loss: 2.2166\n",
      "Epoch [490/2000], Loss: 2.2140\n",
      "Epoch [491/2000], Loss: 2.2130\n",
      "Epoch [492/2000], Loss: 2.2121\n",
      "Epoch [493/2000], Loss: 2.2106\n",
      "Epoch [494/2000], Loss: 2.2073\n",
      "Epoch [495/2000], Loss: 2.2050\n",
      "Epoch [496/2000], Loss: 2.2033\n",
      "Epoch [497/2000], Loss: 2.2020\n",
      "Epoch [498/2000], Loss: 2.2003\n",
      "Epoch [499/2000], Loss: 2.1977\n",
      "Epoch [500/2000], Loss: 2.1957\n",
      "Epoch [501/2000], Loss: 2.1952\n",
      "Epoch [502/2000], Loss: 2.1941\n",
      "Epoch [503/2000], Loss: 2.1916\n",
      "Epoch [504/2000], Loss: 2.1885\n",
      "Epoch [505/2000], Loss: 2.1879\n",
      "Epoch [506/2000], Loss: 2.1889\n",
      "Epoch [507/2000], Loss: 2.1868\n",
      "Epoch [508/2000], Loss: 2.1823\n",
      "Epoch [509/2000], Loss: 2.1801\n",
      "Epoch [510/2000], Loss: 2.1804\n",
      "Epoch [511/2000], Loss: 2.1792\n",
      "Epoch [512/2000], Loss: 2.1746\n",
      "Epoch [513/2000], Loss: 2.1740\n",
      "Epoch [514/2000], Loss: 2.1740\n",
      "Epoch [515/2000], Loss: 2.1704\n",
      "Epoch [516/2000], Loss: 2.1678\n",
      "Epoch [517/2000], Loss: 2.1670\n",
      "Epoch [518/2000], Loss: 2.1652\n",
      "Epoch [519/2000], Loss: 2.1629\n",
      "Epoch [520/2000], Loss: 2.1609\n",
      "Epoch [521/2000], Loss: 2.1596\n",
      "Epoch [522/2000], Loss: 2.1581\n",
      "Epoch [523/2000], Loss: 2.1557\n",
      "Epoch [524/2000], Loss: 2.1544\n",
      "Epoch [525/2000], Loss: 2.1534\n",
      "Epoch [526/2000], Loss: 2.1517\n",
      "Epoch [527/2000], Loss: 2.1492\n",
      "Epoch [528/2000], Loss: 2.1475\n",
      "Epoch [529/2000], Loss: 2.1467\n",
      "Epoch [530/2000], Loss: 2.1460\n",
      "Epoch [531/2000], Loss: 2.1429\n",
      "Epoch [532/2000], Loss: 2.1407\n",
      "Epoch [533/2000], Loss: 2.1404\n",
      "Epoch [534/2000], Loss: 2.1392\n",
      "Epoch [535/2000], Loss: 2.1361\n",
      "Epoch [536/2000], Loss: 2.1343\n",
      "Epoch [537/2000], Loss: 2.1337\n",
      "Epoch [538/2000], Loss: 2.1321\n",
      "Epoch [539/2000], Loss: 2.1293\n",
      "Epoch [540/2000], Loss: 2.1285\n",
      "Epoch [541/2000], Loss: 2.1289\n",
      "Epoch [542/2000], Loss: 2.1258\n",
      "Epoch [543/2000], Loss: 2.1231\n",
      "Epoch [544/2000], Loss: 2.1221\n",
      "Epoch [545/2000], Loss: 2.1211\n",
      "Epoch [546/2000], Loss: 2.1191\n",
      "Epoch [547/2000], Loss: 2.1166\n",
      "Epoch [548/2000], Loss: 2.1157\n",
      "Epoch [549/2000], Loss: 2.1152\n",
      "Epoch [550/2000], Loss: 2.1125\n",
      "Epoch [551/2000], Loss: 2.1105\n",
      "Epoch [552/2000], Loss: 2.1094\n",
      "Epoch [553/2000], Loss: 2.1080\n",
      "Epoch [554/2000], Loss: 2.1065\n",
      "Epoch [555/2000], Loss: 2.1044\n",
      "Epoch [556/2000], Loss: 2.1027\n",
      "Epoch [557/2000], Loss: 2.1015\n",
      "Epoch [558/2000], Loss: 2.0996\n",
      "Epoch [559/2000], Loss: 2.0983\n",
      "Epoch [560/2000], Loss: 2.0968\n",
      "Epoch [561/2000], Loss: 2.0950\n",
      "Epoch [562/2000], Loss: 2.0938\n",
      "Epoch [563/2000], Loss: 2.0926\n",
      "Epoch [564/2000], Loss: 2.0914\n",
      "Epoch [565/2000], Loss: 2.0897\n",
      "Epoch [566/2000], Loss: 2.0877\n",
      "Epoch [567/2000], Loss: 2.0867\n",
      "Epoch [568/2000], Loss: 2.0866\n",
      "Epoch [569/2000], Loss: 2.0846\n",
      "Epoch [570/2000], Loss: 2.0823\n",
      "Epoch [571/2000], Loss: 2.0803\n",
      "Epoch [572/2000], Loss: 2.0799\n",
      "Epoch [573/2000], Loss: 2.0789\n",
      "Epoch [574/2000], Loss: 2.0761\n",
      "Epoch [575/2000], Loss: 2.0744\n",
      "Epoch [576/2000], Loss: 2.0730\n",
      "Epoch [577/2000], Loss: 2.0715\n",
      "Epoch [578/2000], Loss: 2.0700\n",
      "Epoch [579/2000], Loss: 2.0688\n",
      "Epoch [580/2000], Loss: 2.0676\n",
      "Epoch [581/2000], Loss: 2.0657\n",
      "Epoch [582/2000], Loss: 2.0642\n",
      "Epoch [583/2000], Loss: 2.0627\n",
      "Epoch [584/2000], Loss: 2.0615\n",
      "Epoch [585/2000], Loss: 2.0603\n",
      "Epoch [586/2000], Loss: 2.0589\n",
      "Epoch [587/2000], Loss: 2.0575\n",
      "Epoch [588/2000], Loss: 2.0556\n",
      "Epoch [589/2000], Loss: 2.0544\n",
      "Epoch [590/2000], Loss: 2.0529\n",
      "Epoch [591/2000], Loss: 2.0515\n",
      "Epoch [592/2000], Loss: 2.0501\n",
      "Epoch [593/2000], Loss: 2.0487\n",
      "Epoch [594/2000], Loss: 2.0473\n",
      "Epoch [595/2000], Loss: 2.0458\n",
      "Epoch [596/2000], Loss: 2.0446\n",
      "Epoch [597/2000], Loss: 2.0431\n",
      "Epoch [598/2000], Loss: 2.0423\n",
      "Epoch [599/2000], Loss: 2.0414\n",
      "Epoch [600/2000], Loss: 2.0394\n",
      "Epoch [601/2000], Loss: 2.0375\n",
      "Epoch [602/2000], Loss: 2.0370\n",
      "Epoch [603/2000], Loss: 2.0363\n",
      "Epoch [604/2000], Loss: 2.0346\n",
      "Epoch [605/2000], Loss: 2.0324\n",
      "Epoch [606/2000], Loss: 2.0306\n",
      "Epoch [607/2000], Loss: 2.0295\n",
      "Epoch [608/2000], Loss: 2.0283\n",
      "Epoch [609/2000], Loss: 2.0271\n",
      "Epoch [610/2000], Loss: 2.0261\n",
      "Epoch [611/2000], Loss: 2.0242\n",
      "Epoch [612/2000], Loss: 2.0227\n",
      "Epoch [613/2000], Loss: 2.0214\n",
      "Epoch [614/2000], Loss: 2.0199\n",
      "Epoch [615/2000], Loss: 2.0188\n",
      "Epoch [616/2000], Loss: 2.0171\n",
      "Epoch [617/2000], Loss: 2.0160\n",
      "Epoch [618/2000], Loss: 2.0144\n",
      "Epoch [619/2000], Loss: 2.0130\n",
      "Epoch [620/2000], Loss: 2.0115\n",
      "Epoch [621/2000], Loss: 2.0104\n",
      "Epoch [622/2000], Loss: 2.0101\n",
      "Epoch [623/2000], Loss: 2.0086\n",
      "Epoch [624/2000], Loss: 2.0068\n",
      "Epoch [625/2000], Loss: 2.0056\n",
      "Epoch [626/2000], Loss: 2.0040\n",
      "Epoch [627/2000], Loss: 2.0028\n",
      "Epoch [628/2000], Loss: 2.0012\n",
      "Epoch [629/2000], Loss: 2.0003\n",
      "Epoch [630/2000], Loss: 1.9986\n",
      "Epoch [631/2000], Loss: 1.9975\n",
      "Epoch [632/2000], Loss: 1.9965\n",
      "Epoch [633/2000], Loss: 1.9952\n",
      "Epoch [634/2000], Loss: 1.9937\n",
      "Epoch [635/2000], Loss: 1.9921\n",
      "Epoch [636/2000], Loss: 1.9919\n",
      "Epoch [637/2000], Loss: 1.9909\n",
      "Epoch [638/2000], Loss: 1.9894\n",
      "Epoch [639/2000], Loss: 1.9881\n",
      "Epoch [640/2000], Loss: 1.9861\n",
      "Epoch [641/2000], Loss: 1.9846\n",
      "Epoch [642/2000], Loss: 1.9831\n",
      "Epoch [643/2000], Loss: 1.9826\n",
      "Epoch [644/2000], Loss: 1.9811\n",
      "Epoch [645/2000], Loss: 1.9805\n",
      "Epoch [646/2000], Loss: 1.9798\n",
      "Epoch [647/2000], Loss: 1.9783\n",
      "Epoch [648/2000], Loss: 1.9767\n",
      "Epoch [649/2000], Loss: 1.9749\n",
      "Epoch [650/2000], Loss: 1.9737\n",
      "Epoch [651/2000], Loss: 1.9721\n",
      "Epoch [652/2000], Loss: 1.9709\n",
      "Epoch [653/2000], Loss: 1.9695\n",
      "Epoch [654/2000], Loss: 1.9683\n",
      "Epoch [655/2000], Loss: 1.9689\n",
      "Epoch [656/2000], Loss: 1.9687\n",
      "Epoch [657/2000], Loss: 1.9666\n",
      "Epoch [658/2000], Loss: 1.9647\n",
      "Epoch [659/2000], Loss: 1.9622\n",
      "Epoch [660/2000], Loss: 1.9615\n",
      "Epoch [661/2000], Loss: 1.9602\n",
      "Epoch [662/2000], Loss: 1.9594\n",
      "Epoch [663/2000], Loss: 1.9590\n",
      "Epoch [664/2000], Loss: 1.9571\n",
      "Epoch [665/2000], Loss: 1.9570\n",
      "Epoch [666/2000], Loss: 1.9570\n",
      "Epoch [667/2000], Loss: 1.9543\n",
      "Epoch [668/2000], Loss: 1.9521\n",
      "Epoch [669/2000], Loss: 1.9505\n",
      "Epoch [670/2000], Loss: 1.9500\n",
      "Epoch [671/2000], Loss: 1.9507\n",
      "Epoch [672/2000], Loss: 1.9489\n",
      "Epoch [673/2000], Loss: 1.9466\n",
      "Epoch [674/2000], Loss: 1.9448\n",
      "Epoch [675/2000], Loss: 1.9434\n",
      "Epoch [676/2000], Loss: 1.9421\n",
      "Epoch [677/2000], Loss: 1.9410\n",
      "Epoch [678/2000], Loss: 1.9405\n",
      "Epoch [679/2000], Loss: 1.9394\n",
      "Epoch [680/2000], Loss: 1.9380\n",
      "Epoch [681/2000], Loss: 1.9365\n",
      "Epoch [682/2000], Loss: 1.9350\n",
      "Epoch [683/2000], Loss: 1.9340\n",
      "Epoch [684/2000], Loss: 1.9326\n",
      "Epoch [685/2000], Loss: 1.9315\n",
      "Epoch [686/2000], Loss: 1.9303\n",
      "Epoch [687/2000], Loss: 1.9299\n",
      "Epoch [688/2000], Loss: 1.9304\n",
      "Epoch [689/2000], Loss: 1.9311\n",
      "Epoch [690/2000], Loss: 1.9288\n",
      "Epoch [691/2000], Loss: 1.9254\n",
      "Epoch [692/2000], Loss: 1.9238\n",
      "Epoch [693/2000], Loss: 1.9245\n",
      "Epoch [694/2000], Loss: 1.9238\n",
      "Epoch [695/2000], Loss: 1.9213\n",
      "Epoch [696/2000], Loss: 1.9194\n",
      "Epoch [697/2000], Loss: 1.9182\n",
      "Epoch [698/2000], Loss: 1.9173\n",
      "Epoch [699/2000], Loss: 1.9166\n",
      "Epoch [700/2000], Loss: 1.9160\n",
      "Epoch [701/2000], Loss: 1.9149\n",
      "Epoch [702/2000], Loss: 1.9133\n",
      "Epoch [703/2000], Loss: 1.9114\n",
      "Epoch [704/2000], Loss: 1.9112\n",
      "Epoch [705/2000], Loss: 1.9119\n",
      "Epoch [706/2000], Loss: 1.9103\n",
      "Epoch [707/2000], Loss: 1.9076\n",
      "Epoch [708/2000], Loss: 1.9062\n",
      "Epoch [709/2000], Loss: 1.9056\n",
      "Epoch [710/2000], Loss: 1.9060\n",
      "Epoch [711/2000], Loss: 1.9039\n",
      "Epoch [712/2000], Loss: 1.9015\n",
      "Epoch [713/2000], Loss: 1.9008\n",
      "Epoch [714/2000], Loss: 1.9002\n",
      "Epoch [715/2000], Loss: 1.8996\n",
      "Epoch [716/2000], Loss: 1.8984\n",
      "Epoch [717/2000], Loss: 1.8968\n",
      "Epoch [718/2000], Loss: 1.8957\n",
      "Epoch [719/2000], Loss: 1.8940\n",
      "Epoch [720/2000], Loss: 1.8948\n",
      "Epoch [721/2000], Loss: 1.8949\n",
      "Epoch [722/2000], Loss: 1.8939\n",
      "Epoch [723/2000], Loss: 1.8908\n",
      "Epoch [724/2000], Loss: 1.8889\n",
      "Epoch [725/2000], Loss: 1.8882\n",
      "Epoch [726/2000], Loss: 1.8870\n",
      "Epoch [727/2000], Loss: 1.8855\n",
      "Epoch [728/2000], Loss: 1.8849\n",
      "Epoch [729/2000], Loss: 1.8836\n",
      "Epoch [730/2000], Loss: 1.8829\n",
      "Epoch [731/2000], Loss: 1.8823\n",
      "Epoch [732/2000], Loss: 1.8827\n",
      "Epoch [733/2000], Loss: 1.8820\n",
      "Epoch [734/2000], Loss: 1.8791\n",
      "Epoch [735/2000], Loss: 1.8776\n",
      "Epoch [736/2000], Loss: 1.8773\n",
      "Epoch [737/2000], Loss: 1.8766\n",
      "Epoch [738/2000], Loss: 1.8749\n",
      "Epoch [739/2000], Loss: 1.8734\n",
      "Epoch [740/2000], Loss: 1.8722\n",
      "Epoch [741/2000], Loss: 1.8709\n",
      "Epoch [742/2000], Loss: 1.8698\n",
      "Epoch [743/2000], Loss: 1.8699\n",
      "Epoch [744/2000], Loss: 1.8698\n",
      "Epoch [745/2000], Loss: 1.8687\n",
      "Epoch [746/2000], Loss: 1.8668\n",
      "Epoch [747/2000], Loss: 1.8649\n",
      "Epoch [748/2000], Loss: 1.8651\n",
      "Epoch [749/2000], Loss: 1.8656\n",
      "Epoch [750/2000], Loss: 1.8647\n",
      "Epoch [751/2000], Loss: 1.8613\n",
      "Epoch [752/2000], Loss: 1.8607\n",
      "Epoch [753/2000], Loss: 1.8645\n",
      "Epoch [754/2000], Loss: 1.8641\n",
      "Epoch [755/2000], Loss: 1.8599\n",
      "Epoch [756/2000], Loss: 1.8563\n",
      "Epoch [757/2000], Loss: 1.8572\n",
      "Epoch [758/2000], Loss: 1.8580\n",
      "Epoch [759/2000], Loss: 1.8541\n",
      "Epoch [760/2000], Loss: 1.8522\n",
      "Epoch [761/2000], Loss: 1.8527\n",
      "Epoch [762/2000], Loss: 1.8515\n",
      "Epoch [763/2000], Loss: 1.8496\n",
      "Epoch [764/2000], Loss: 1.8481\n",
      "Epoch [765/2000], Loss: 1.8482\n",
      "Epoch [766/2000], Loss: 1.8483\n",
      "Epoch [767/2000], Loss: 1.8468\n",
      "Epoch [768/2000], Loss: 1.8455\n",
      "Epoch [769/2000], Loss: 1.8436\n",
      "Epoch [770/2000], Loss: 1.8432\n",
      "Epoch [771/2000], Loss: 1.8447\n",
      "Epoch [772/2000], Loss: 1.8436\n",
      "Epoch [773/2000], Loss: 1.8416\n",
      "Epoch [774/2000], Loss: 1.8389\n",
      "Epoch [775/2000], Loss: 1.8399\n",
      "Epoch [776/2000], Loss: 1.8388\n",
      "Epoch [777/2000], Loss: 1.8363\n",
      "Epoch [778/2000], Loss: 1.8354\n",
      "Epoch [779/2000], Loss: 1.8345\n",
      "Epoch [780/2000], Loss: 1.8342\n",
      "Epoch [781/2000], Loss: 1.8340\n",
      "Epoch [782/2000], Loss: 1.8320\n",
      "Epoch [783/2000], Loss: 1.8302\n",
      "Epoch [784/2000], Loss: 1.8299\n",
      "Epoch [785/2000], Loss: 1.8303\n",
      "Epoch [786/2000], Loss: 1.8306\n",
      "Epoch [787/2000], Loss: 1.8276\n",
      "Epoch [788/2000], Loss: 1.8258\n",
      "Epoch [789/2000], Loss: 1.8254\n",
      "Epoch [790/2000], Loss: 1.8254\n",
      "Epoch [791/2000], Loss: 1.8246\n",
      "Epoch [792/2000], Loss: 1.8223\n",
      "Epoch [793/2000], Loss: 1.8217\n",
      "Epoch [794/2000], Loss: 1.8217\n",
      "Epoch [795/2000], Loss: 1.8207\n",
      "Epoch [796/2000], Loss: 1.8188\n",
      "Epoch [797/2000], Loss: 1.8180\n",
      "Epoch [798/2000], Loss: 1.8166\n",
      "Epoch [799/2000], Loss: 1.8164\n",
      "Epoch [800/2000], Loss: 1.8150\n",
      "Epoch [801/2000], Loss: 1.8147\n",
      "Epoch [802/2000], Loss: 1.8130\n",
      "Epoch [803/2000], Loss: 1.8128\n",
      "Epoch [804/2000], Loss: 1.8134\n",
      "Epoch [805/2000], Loss: 1.8138\n",
      "Epoch [806/2000], Loss: 1.8114\n",
      "Epoch [807/2000], Loss: 1.8085\n",
      "Epoch [808/2000], Loss: 1.8096\n",
      "Epoch [809/2000], Loss: 1.8115\n",
      "Epoch [810/2000], Loss: 1.8087\n",
      "Epoch [811/2000], Loss: 1.8054\n",
      "Epoch [812/2000], Loss: 1.8061\n",
      "Epoch [813/2000], Loss: 1.8066\n",
      "Epoch [814/2000], Loss: 1.8043\n",
      "Epoch [815/2000], Loss: 1.8019\n",
      "Epoch [816/2000], Loss: 1.8012\n",
      "Epoch [817/2000], Loss: 1.8025\n",
      "Epoch [818/2000], Loss: 1.8015\n",
      "Epoch [819/2000], Loss: 1.7981\n",
      "Epoch [820/2000], Loss: 1.7976\n",
      "Epoch [821/2000], Loss: 1.7972\n",
      "Epoch [822/2000], Loss: 1.7964\n",
      "Epoch [823/2000], Loss: 1.7955\n",
      "Epoch [824/2000], Loss: 1.7938\n",
      "Epoch [825/2000], Loss: 1.7931\n",
      "Epoch [826/2000], Loss: 1.7929\n",
      "Epoch [827/2000], Loss: 1.7928\n",
      "Epoch [828/2000], Loss: 1.7916\n",
      "Epoch [829/2000], Loss: 1.7904\n",
      "Epoch [830/2000], Loss: 1.7890\n",
      "Epoch [831/2000], Loss: 1.7885\n",
      "Epoch [832/2000], Loss: 1.7882\n",
      "Epoch [833/2000], Loss: 1.7875\n",
      "Epoch [834/2000], Loss: 1.7873\n",
      "Epoch [835/2000], Loss: 1.7846\n",
      "Epoch [836/2000], Loss: 1.7837\n",
      "Epoch [837/2000], Loss: 1.7828\n",
      "Epoch [838/2000], Loss: 1.7818\n",
      "Epoch [839/2000], Loss: 1.7811\n",
      "Epoch [840/2000], Loss: 1.7800\n",
      "Epoch [841/2000], Loss: 1.7793\n",
      "Epoch [842/2000], Loss: 1.7793\n",
      "Epoch [843/2000], Loss: 1.7813\n",
      "Epoch [844/2000], Loss: 1.7800\n",
      "Epoch [845/2000], Loss: 1.7763\n",
      "Epoch [846/2000], Loss: 1.7755\n",
      "Epoch [847/2000], Loss: 1.7769\n",
      "Epoch [848/2000], Loss: 1.7766\n",
      "Epoch [849/2000], Loss: 1.7731\n",
      "Epoch [850/2000], Loss: 1.7724\n",
      "Epoch [851/2000], Loss: 1.7741\n",
      "Epoch [852/2000], Loss: 1.7729\n",
      "Epoch [853/2000], Loss: 1.7706\n",
      "Epoch [854/2000], Loss: 1.7690\n",
      "Epoch [855/2000], Loss: 1.7693\n",
      "Epoch [856/2000], Loss: 1.7689\n",
      "Epoch [857/2000], Loss: 1.7669\n",
      "Epoch [858/2000], Loss: 1.7657\n",
      "Epoch [859/2000], Loss: 1.7691\n",
      "Epoch [860/2000], Loss: 1.7691\n",
      "Epoch [861/2000], Loss: 1.7636\n",
      "Epoch [862/2000], Loss: 1.7640\n",
      "Epoch [863/2000], Loss: 1.7678\n",
      "Epoch [864/2000], Loss: 1.7635\n",
      "Epoch [865/2000], Loss: 1.7603\n",
      "Epoch [866/2000], Loss: 1.7608\n",
      "Epoch [867/2000], Loss: 1.7608\n",
      "Epoch [868/2000], Loss: 1.7586\n",
      "Epoch [869/2000], Loss: 1.7566\n",
      "Epoch [870/2000], Loss: 1.7600\n",
      "Epoch [871/2000], Loss: 1.7598\n",
      "Epoch [872/2000], Loss: 1.7548\n",
      "Epoch [873/2000], Loss: 1.7549\n",
      "Epoch [874/2000], Loss: 1.7581\n",
      "Epoch [875/2000], Loss: 1.7540\n",
      "Epoch [876/2000], Loss: 1.7514\n",
      "Epoch [877/2000], Loss: 1.7517\n",
      "Epoch [878/2000], Loss: 1.7506\n",
      "Epoch [879/2000], Loss: 1.7492\n",
      "Epoch [880/2000], Loss: 1.7478\n",
      "Epoch [881/2000], Loss: 1.7478\n",
      "Epoch [882/2000], Loss: 1.7482\n",
      "Epoch [883/2000], Loss: 1.7470\n",
      "Epoch [884/2000], Loss: 1.7446\n",
      "Epoch [885/2000], Loss: 1.7457\n",
      "Epoch [886/2000], Loss: 1.7449\n",
      "Epoch [887/2000], Loss: 1.7437\n",
      "Epoch [888/2000], Loss: 1.7428\n",
      "Epoch [889/2000], Loss: 1.7417\n",
      "Epoch [890/2000], Loss: 1.7411\n",
      "Epoch [891/2000], Loss: 1.7401\n",
      "Epoch [892/2000], Loss: 1.7415\n",
      "Epoch [893/2000], Loss: 1.7395\n",
      "Epoch [894/2000], Loss: 1.7379\n",
      "Epoch [895/2000], Loss: 1.7367\n",
      "Epoch [896/2000], Loss: 1.7390\n",
      "Epoch [897/2000], Loss: 1.7368\n",
      "Epoch [898/2000], Loss: 1.7349\n",
      "Epoch [899/2000], Loss: 1.7348\n",
      "Epoch [900/2000], Loss: 1.7337\n",
      "Epoch [901/2000], Loss: 1.7338\n",
      "Epoch [902/2000], Loss: 1.7315\n",
      "Epoch [903/2000], Loss: 1.7313\n",
      "Epoch [904/2000], Loss: 1.7298\n",
      "Epoch [905/2000], Loss: 1.7303\n",
      "Epoch [906/2000], Loss: 1.7295\n",
      "Epoch [907/2000], Loss: 1.7283\n",
      "Epoch [908/2000], Loss: 1.7274\n",
      "Epoch [909/2000], Loss: 1.7259\n",
      "Epoch [910/2000], Loss: 1.7250\n",
      "Epoch [911/2000], Loss: 1.7248\n",
      "Epoch [912/2000], Loss: 1.7235\n",
      "Epoch [913/2000], Loss: 1.7232\n",
      "Epoch [914/2000], Loss: 1.7230\n",
      "Epoch [915/2000], Loss: 1.7215\n",
      "Epoch [916/2000], Loss: 1.7213\n",
      "Epoch [917/2000], Loss: 1.7200\n",
      "Epoch [918/2000], Loss: 1.7200\n",
      "Epoch [919/2000], Loss: 1.7189\n",
      "Epoch [920/2000], Loss: 1.7177\n",
      "Epoch [921/2000], Loss: 1.7175\n",
      "Epoch [922/2000], Loss: 1.7171\n",
      "Epoch [923/2000], Loss: 1.7158\n",
      "Epoch [924/2000], Loss: 1.7147\n",
      "Epoch [925/2000], Loss: 1.7145\n",
      "Epoch [926/2000], Loss: 1.7144\n",
      "Epoch [927/2000], Loss: 1.7147\n",
      "Epoch [928/2000], Loss: 1.7128\n",
      "Epoch [929/2000], Loss: 1.7114\n",
      "Epoch [930/2000], Loss: 1.7122\n",
      "Epoch [931/2000], Loss: 1.7126\n",
      "Epoch [932/2000], Loss: 1.7109\n",
      "Epoch [933/2000], Loss: 1.7085\n",
      "Epoch [934/2000], Loss: 1.7096\n",
      "Epoch [935/2000], Loss: 1.7113\n",
      "Epoch [936/2000], Loss: 1.7088\n",
      "Epoch [937/2000], Loss: 1.7060\n",
      "Epoch [938/2000], Loss: 1.7061\n",
      "Epoch [939/2000], Loss: 1.7095\n",
      "Epoch [940/2000], Loss: 1.7065\n",
      "Epoch [941/2000], Loss: 1.7029\n",
      "Epoch [942/2000], Loss: 1.7055\n",
      "Epoch [943/2000], Loss: 1.7049\n",
      "Epoch [944/2000], Loss: 1.7014\n",
      "Epoch [945/2000], Loss: 1.7006\n",
      "Epoch [946/2000], Loss: 1.7013\n",
      "Epoch [947/2000], Loss: 1.7008\n",
      "Epoch [948/2000], Loss: 1.6984\n",
      "Epoch [949/2000], Loss: 1.6971\n",
      "Epoch [950/2000], Loss: 1.6979\n",
      "Epoch [951/2000], Loss: 1.6965\n",
      "Epoch [952/2000], Loss: 1.6958\n",
      "Epoch [953/2000], Loss: 1.6948\n",
      "Epoch [954/2000], Loss: 1.6936\n",
      "Epoch [955/2000], Loss: 1.6935\n",
      "Epoch [956/2000], Loss: 1.6945\n",
      "Epoch [957/2000], Loss: 1.6921\n",
      "Epoch [958/2000], Loss: 1.6907\n",
      "Epoch [959/2000], Loss: 1.6899\n",
      "Epoch [960/2000], Loss: 1.6890\n",
      "Epoch [961/2000], Loss: 1.6888\n",
      "Epoch [962/2000], Loss: 1.6890\n",
      "Epoch [963/2000], Loss: 1.6886\n",
      "Epoch [964/2000], Loss: 1.6884\n",
      "Epoch [965/2000], Loss: 1.6863\n",
      "Epoch [966/2000], Loss: 1.6859\n",
      "Epoch [967/2000], Loss: 1.6861\n",
      "Epoch [968/2000], Loss: 1.6857\n",
      "Epoch [969/2000], Loss: 1.6838\n",
      "Epoch [970/2000], Loss: 1.6827\n",
      "Epoch [971/2000], Loss: 1.6821\n",
      "Epoch [972/2000], Loss: 1.6841\n",
      "Epoch [973/2000], Loss: 1.6829\n",
      "Epoch [974/2000], Loss: 1.6797\n",
      "Epoch [975/2000], Loss: 1.6798\n",
      "Epoch [976/2000], Loss: 1.6824\n",
      "Epoch [977/2000], Loss: 1.6806\n",
      "Epoch [978/2000], Loss: 1.6772\n",
      "Epoch [979/2000], Loss: 1.6771\n",
      "Epoch [980/2000], Loss: 1.6769\n",
      "Epoch [981/2000], Loss: 1.6771\n",
      "Epoch [982/2000], Loss: 1.6747\n",
      "Epoch [983/2000], Loss: 1.6746\n",
      "Epoch [984/2000], Loss: 1.6753\n",
      "Epoch [985/2000], Loss: 1.6757\n",
      "Epoch [986/2000], Loss: 1.6724\n",
      "Epoch [987/2000], Loss: 1.6731\n",
      "Epoch [988/2000], Loss: 1.6737\n",
      "Epoch [989/2000], Loss: 1.6716\n",
      "Epoch [990/2000], Loss: 1.6694\n",
      "Epoch [991/2000], Loss: 1.6690\n",
      "Epoch [992/2000], Loss: 1.6701\n",
      "Epoch [993/2000], Loss: 1.6700\n",
      "Epoch [994/2000], Loss: 1.6681\n",
      "Epoch [995/2000], Loss: 1.6661\n",
      "Epoch [996/2000], Loss: 1.6658\n",
      "Epoch [997/2000], Loss: 1.6671\n",
      "Epoch [998/2000], Loss: 1.6656\n",
      "Epoch [999/2000], Loss: 1.6635\n",
      "Epoch [1000/2000], Loss: 1.6637\n",
      "Epoch [1001/2000], Loss: 1.6676\n",
      "Epoch [1002/2000], Loss: 1.6640\n",
      "Epoch [1003/2000], Loss: 1.6612\n",
      "Epoch [1004/2000], Loss: 1.6614\n",
      "Epoch [1005/2000], Loss: 1.6626\n",
      "Epoch [1006/2000], Loss: 1.6596\n",
      "Epoch [1007/2000], Loss: 1.6583\n",
      "Epoch [1008/2000], Loss: 1.6586\n",
      "Epoch [1009/2000], Loss: 1.6580\n",
      "Epoch [1010/2000], Loss: 1.6571\n",
      "Epoch [1011/2000], Loss: 1.6557\n",
      "Epoch [1012/2000], Loss: 1.6572\n",
      "Epoch [1013/2000], Loss: 1.6570\n",
      "Epoch [1014/2000], Loss: 1.6552\n",
      "Epoch [1015/2000], Loss: 1.6536\n",
      "Epoch [1016/2000], Loss: 1.6549\n",
      "Epoch [1017/2000], Loss: 1.6538\n",
      "Epoch [1018/2000], Loss: 1.6525\n",
      "Epoch [1019/2000], Loss: 1.6512\n",
      "Epoch [1020/2000], Loss: 1.6512\n",
      "Epoch [1021/2000], Loss: 1.6513\n",
      "Epoch [1022/2000], Loss: 1.6503\n",
      "Epoch [1023/2000], Loss: 1.6486\n",
      "Epoch [1024/2000], Loss: 1.6483\n",
      "Epoch [1025/2000], Loss: 1.6492\n",
      "Epoch [1026/2000], Loss: 1.6471\n",
      "Epoch [1027/2000], Loss: 1.6457\n",
      "Epoch [1028/2000], Loss: 1.6470\n",
      "Epoch [1029/2000], Loss: 1.6487\n",
      "Epoch [1030/2000], Loss: 1.6443\n",
      "Epoch [1031/2000], Loss: 1.6430\n",
      "Epoch [1032/2000], Loss: 1.6437\n",
      "Epoch [1033/2000], Loss: 1.6447\n",
      "Epoch [1034/2000], Loss: 1.6424\n",
      "Epoch [1035/2000], Loss: 1.6415\n",
      "Epoch [1036/2000], Loss: 1.6434\n",
      "Epoch [1037/2000], Loss: 1.6408\n",
      "Epoch [1038/2000], Loss: 1.6389\n",
      "Epoch [1039/2000], Loss: 1.6388\n",
      "Epoch [1040/2000], Loss: 1.6403\n",
      "Epoch [1041/2000], Loss: 1.6393\n",
      "Epoch [1042/2000], Loss: 1.6368\n",
      "Epoch [1043/2000], Loss: 1.6358\n",
      "Epoch [1044/2000], Loss: 1.6372\n",
      "Epoch [1045/2000], Loss: 1.6356\n",
      "Epoch [1046/2000], Loss: 1.6337\n",
      "Epoch [1047/2000], Loss: 1.6335\n",
      "Epoch [1048/2000], Loss: 1.6341\n",
      "Epoch [1049/2000], Loss: 1.6334\n",
      "Epoch [1050/2000], Loss: 1.6331\n",
      "Epoch [1051/2000], Loss: 1.6322\n",
      "Epoch [1052/2000], Loss: 1.6304\n",
      "Epoch [1053/2000], Loss: 1.6306\n",
      "Epoch [1054/2000], Loss: 1.6302\n",
      "Epoch [1055/2000], Loss: 1.6287\n",
      "Epoch [1056/2000], Loss: 1.6279\n",
      "Epoch [1057/2000], Loss: 1.6281\n",
      "Epoch [1058/2000], Loss: 1.6280\n",
      "Epoch [1059/2000], Loss: 1.6270\n",
      "Epoch [1060/2000], Loss: 1.6255\n",
      "Epoch [1061/2000], Loss: 1.6258\n",
      "Epoch [1062/2000], Loss: 1.6269\n",
      "Epoch [1063/2000], Loss: 1.6259\n",
      "Epoch [1064/2000], Loss: 1.6234\n",
      "Epoch [1065/2000], Loss: 1.6230\n",
      "Epoch [1066/2000], Loss: 1.6240\n",
      "Epoch [1067/2000], Loss: 1.6232\n",
      "Epoch [1068/2000], Loss: 1.6214\n",
      "Epoch [1069/2000], Loss: 1.6209\n",
      "Epoch [1070/2000], Loss: 1.6217\n",
      "Epoch [1071/2000], Loss: 1.6206\n",
      "Epoch [1072/2000], Loss: 1.6189\n",
      "Epoch [1073/2000], Loss: 1.6194\n",
      "Epoch [1074/2000], Loss: 1.6206\n",
      "Epoch [1075/2000], Loss: 1.6174\n",
      "Epoch [1076/2000], Loss: 1.6166\n",
      "Epoch [1077/2000], Loss: 1.6212\n",
      "Epoch [1078/2000], Loss: 1.6179\n",
      "Epoch [1079/2000], Loss: 1.6140\n",
      "Epoch [1080/2000], Loss: 1.6149\n",
      "Epoch [1081/2000], Loss: 1.6177\n",
      "Epoch [1082/2000], Loss: 1.6142\n",
      "Epoch [1083/2000], Loss: 1.6133\n",
      "Epoch [1084/2000], Loss: 1.6143\n",
      "Epoch [1085/2000], Loss: 1.6120\n",
      "Epoch [1086/2000], Loss: 1.6106\n",
      "Epoch [1087/2000], Loss: 1.6098\n",
      "Epoch [1088/2000], Loss: 1.6101\n",
      "Epoch [1089/2000], Loss: 1.6109\n",
      "Epoch [1090/2000], Loss: 1.6085\n",
      "Epoch [1091/2000], Loss: 1.6080\n",
      "Epoch [1092/2000], Loss: 1.6094\n",
      "Epoch [1093/2000], Loss: 1.6087\n",
      "Epoch [1094/2000], Loss: 1.6067\n",
      "Epoch [1095/2000], Loss: 1.6071\n",
      "Epoch [1096/2000], Loss: 1.6063\n",
      "Epoch [1097/2000], Loss: 1.6048\n",
      "Epoch [1098/2000], Loss: 1.6040\n",
      "Epoch [1099/2000], Loss: 1.6027\n",
      "Epoch [1100/2000], Loss: 1.6023\n",
      "Epoch [1101/2000], Loss: 1.6033\n",
      "Epoch [1102/2000], Loss: 1.6038\n",
      "Epoch [1103/2000], Loss: 1.6030\n",
      "Epoch [1104/2000], Loss: 1.6012\n",
      "Epoch [1105/2000], Loss: 1.6015\n",
      "Epoch [1106/2000], Loss: 1.6007\n",
      "Epoch [1107/2000], Loss: 1.5994\n",
      "Epoch [1108/2000], Loss: 1.5985\n",
      "Epoch [1109/2000], Loss: 1.6010\n",
      "Epoch [1110/2000], Loss: 1.5998\n",
      "Epoch [1111/2000], Loss: 1.5970\n",
      "Epoch [1112/2000], Loss: 1.5991\n",
      "Epoch [1113/2000], Loss: 1.5984\n",
      "Epoch [1114/2000], Loss: 1.5950\n",
      "Epoch [1115/2000], Loss: 1.5947\n",
      "Epoch [1116/2000], Loss: 1.5958\n",
      "Epoch [1117/2000], Loss: 1.5929\n",
      "Epoch [1118/2000], Loss: 1.5950\n",
      "Epoch [1119/2000], Loss: 1.5934\n",
      "Epoch [1120/2000], Loss: 1.5933\n",
      "Epoch [1121/2000], Loss: 1.5914\n",
      "Epoch [1122/2000], Loss: 1.5917\n",
      "Epoch [1123/2000], Loss: 1.5900\n",
      "Epoch [1124/2000], Loss: 1.5896\n",
      "Epoch [1125/2000], Loss: 1.5881\n",
      "Epoch [1126/2000], Loss: 1.5883\n",
      "Epoch [1127/2000], Loss: 1.5882\n",
      "Epoch [1128/2000], Loss: 1.5878\n",
      "Epoch [1129/2000], Loss: 1.5864\n",
      "Epoch [1130/2000], Loss: 1.5857\n",
      "Epoch [1131/2000], Loss: 1.5851\n",
      "Epoch [1132/2000], Loss: 1.5851\n",
      "Epoch [1133/2000], Loss: 1.5842\n",
      "Epoch [1134/2000], Loss: 1.5832\n",
      "Epoch [1135/2000], Loss: 1.5838\n",
      "Epoch [1136/2000], Loss: 1.5852\n",
      "Epoch [1137/2000], Loss: 1.5839\n",
      "Epoch [1138/2000], Loss: 1.5816\n",
      "Epoch [1139/2000], Loss: 1.5814\n",
      "Epoch [1140/2000], Loss: 1.5807\n",
      "Epoch [1141/2000], Loss: 1.5794\n",
      "Epoch [1142/2000], Loss: 1.5804\n",
      "Epoch [1143/2000], Loss: 1.5817\n",
      "Epoch [1144/2000], Loss: 1.5808\n",
      "Epoch [1145/2000], Loss: 1.5785\n",
      "Epoch [1146/2000], Loss: 1.5771\n",
      "Epoch [1147/2000], Loss: 1.5782\n",
      "Epoch [1148/2000], Loss: 1.5773\n",
      "Epoch [1149/2000], Loss: 1.5769\n",
      "Epoch [1150/2000], Loss: 1.5749\n",
      "Epoch [1151/2000], Loss: 1.5766\n",
      "Epoch [1152/2000], Loss: 1.5757\n",
      "Epoch [1153/2000], Loss: 1.5741\n",
      "Epoch [1154/2000], Loss: 1.5731\n",
      "Epoch [1155/2000], Loss: 1.5746\n",
      "Epoch [1156/2000], Loss: 1.5739\n",
      "Epoch [1157/2000], Loss: 1.5723\n",
      "Epoch [1158/2000], Loss: 1.5725\n",
      "Epoch [1159/2000], Loss: 1.5708\n",
      "Epoch [1160/2000], Loss: 1.5704\n",
      "Epoch [1161/2000], Loss: 1.5696\n",
      "Epoch [1162/2000], Loss: 1.5706\n",
      "Epoch [1163/2000], Loss: 1.5704\n",
      "Epoch [1164/2000], Loss: 1.5686\n",
      "Epoch [1165/2000], Loss: 1.5673\n",
      "Epoch [1166/2000], Loss: 1.5694\n",
      "Epoch [1167/2000], Loss: 1.5694\n",
      "Epoch [1168/2000], Loss: 1.5664\n",
      "Epoch [1169/2000], Loss: 1.5654\n",
      "Epoch [1170/2000], Loss: 1.5655\n",
      "Epoch [1171/2000], Loss: 1.5645\n",
      "Epoch [1172/2000], Loss: 1.5639\n",
      "Epoch [1173/2000], Loss: 1.5637\n",
      "Epoch [1174/2000], Loss: 1.5625\n",
      "Epoch [1175/2000], Loss: 1.5627\n",
      "Epoch [1176/2000], Loss: 1.5626\n",
      "Epoch [1177/2000], Loss: 1.5623\n",
      "Epoch [1178/2000], Loss: 1.5605\n",
      "Epoch [1179/2000], Loss: 1.5614\n",
      "Epoch [1180/2000], Loss: 1.5614\n",
      "Epoch [1181/2000], Loss: 1.5603\n",
      "Epoch [1182/2000], Loss: 1.5585\n",
      "Epoch [1183/2000], Loss: 1.5609\n",
      "Epoch [1184/2000], Loss: 1.5600\n",
      "Epoch [1185/2000], Loss: 1.5578\n",
      "Epoch [1186/2000], Loss: 1.5580\n",
      "Epoch [1187/2000], Loss: 1.5563\n",
      "Epoch [1188/2000], Loss: 1.5560\n",
      "Epoch [1189/2000], Loss: 1.5555\n",
      "Epoch [1190/2000], Loss: 1.5560\n",
      "Epoch [1191/2000], Loss: 1.5557\n",
      "Epoch [1192/2000], Loss: 1.5542\n",
      "Epoch [1193/2000], Loss: 1.5540\n",
      "Epoch [1194/2000], Loss: 1.5521\n",
      "Epoch [1195/2000], Loss: 1.5531\n",
      "Epoch [1196/2000], Loss: 1.5523\n",
      "Epoch [1197/2000], Loss: 1.5512\n",
      "Epoch [1198/2000], Loss: 1.5505\n",
      "Epoch [1199/2000], Loss: 1.5500\n",
      "Epoch [1200/2000], Loss: 1.5503\n",
      "Epoch [1201/2000], Loss: 1.5505\n",
      "Epoch [1202/2000], Loss: 1.5490\n",
      "Epoch [1203/2000], Loss: 1.5481\n",
      "Epoch [1204/2000], Loss: 1.5504\n",
      "Epoch [1205/2000], Loss: 1.5495\n",
      "Epoch [1206/2000], Loss: 1.5466\n",
      "Epoch [1207/2000], Loss: 1.5468\n",
      "Epoch [1208/2000], Loss: 1.5459\n",
      "Epoch [1209/2000], Loss: 1.5447\n",
      "Epoch [1210/2000], Loss: 1.5448\n",
      "Epoch [1211/2000], Loss: 1.5468\n",
      "Epoch [1212/2000], Loss: 1.5460\n",
      "Epoch [1213/2000], Loss: 1.5438\n",
      "Epoch [1214/2000], Loss: 1.5426\n",
      "Epoch [1215/2000], Loss: 1.5429\n",
      "Epoch [1216/2000], Loss: 1.5425\n",
      "Epoch [1217/2000], Loss: 1.5408\n",
      "Epoch [1218/2000], Loss: 1.5427\n",
      "Epoch [1219/2000], Loss: 1.5440\n",
      "Epoch [1220/2000], Loss: 1.5405\n",
      "Epoch [1221/2000], Loss: 1.5407\n",
      "Epoch [1222/2000], Loss: 1.5462\n",
      "Epoch [1223/2000], Loss: 1.5401\n",
      "Epoch [1224/2000], Loss: 1.5396\n",
      "Epoch [1225/2000], Loss: 1.5476\n",
      "Epoch [1226/2000], Loss: 1.5389\n",
      "Epoch [1227/2000], Loss: 1.5393\n",
      "Epoch [1228/2000], Loss: 1.5451\n",
      "Epoch [1229/2000], Loss: 1.5377\n",
      "Epoch [1230/2000], Loss: 1.5383\n",
      "Epoch [1231/2000], Loss: 1.5444\n",
      "Epoch [1232/2000], Loss: 1.5352\n",
      "Epoch [1233/2000], Loss: 1.5395\n",
      "Epoch [1234/2000], Loss: 1.5443\n",
      "Epoch [1235/2000], Loss: 1.5331\n",
      "Epoch [1236/2000], Loss: 1.5412\n",
      "Epoch [1237/2000], Loss: 1.5426\n",
      "Epoch [1238/2000], Loss: 1.5325\n",
      "Epoch [1239/2000], Loss: 1.5421\n",
      "Epoch [1240/2000], Loss: 1.5373\n",
      "Epoch [1241/2000], Loss: 1.5321\n",
      "Epoch [1242/2000], Loss: 1.5387\n",
      "Epoch [1243/2000], Loss: 1.5310\n",
      "Epoch [1244/2000], Loss: 1.5328\n",
      "Epoch [1245/2000], Loss: 1.5325\n",
      "Epoch [1246/2000], Loss: 1.5294\n",
      "Epoch [1247/2000], Loss: 1.5309\n",
      "Epoch [1248/2000], Loss: 1.5274\n",
      "Epoch [1249/2000], Loss: 1.5277\n",
      "Epoch [1250/2000], Loss: 1.5272\n",
      "Epoch [1251/2000], Loss: 1.5241\n",
      "Epoch [1252/2000], Loss: 1.5278\n",
      "Epoch [1253/2000], Loss: 1.5262\n",
      "Epoch [1254/2000], Loss: 1.5250\n",
      "Epoch [1255/2000], Loss: 1.5259\n",
      "Epoch [1256/2000], Loss: 1.5230\n",
      "Epoch [1257/2000], Loss: 1.5241\n",
      "Epoch [1258/2000], Loss: 1.5214\n",
      "Epoch [1259/2000], Loss: 1.5209\n",
      "Epoch [1260/2000], Loss: 1.5213\n",
      "Epoch [1261/2000], Loss: 1.5193\n",
      "Epoch [1262/2000], Loss: 1.5214\n",
      "Epoch [1263/2000], Loss: 1.5219\n",
      "Epoch [1264/2000], Loss: 1.5203\n",
      "Epoch [1265/2000], Loss: 1.5188\n",
      "Epoch [1266/2000], Loss: 1.5199\n",
      "Epoch [1267/2000], Loss: 1.5214\n",
      "Epoch [1268/2000], Loss: 1.5171\n",
      "Epoch [1269/2000], Loss: 1.5202\n",
      "Epoch [1270/2000], Loss: 1.5184\n",
      "Epoch [1271/2000], Loss: 1.5153\n",
      "Epoch [1272/2000], Loss: 1.5195\n",
      "Epoch [1273/2000], Loss: 1.5167\n",
      "Epoch [1274/2000], Loss: 1.5137\n",
      "Epoch [1275/2000], Loss: 1.5177\n",
      "Epoch [1276/2000], Loss: 1.5151\n",
      "Epoch [1277/2000], Loss: 1.5130\n",
      "Epoch [1278/2000], Loss: 1.5150\n",
      "Epoch [1279/2000], Loss: 1.5136\n",
      "Epoch [1280/2000], Loss: 1.5118\n",
      "Epoch [1281/2000], Loss: 1.5127\n",
      "Epoch [1282/2000], Loss: 1.5127\n",
      "Epoch [1283/2000], Loss: 1.5101\n",
      "Epoch [1284/2000], Loss: 1.5114\n",
      "Epoch [1285/2000], Loss: 1.5114\n",
      "Epoch [1286/2000], Loss: 1.5085\n",
      "Epoch [1287/2000], Loss: 1.5101\n",
      "Epoch [1288/2000], Loss: 1.5089\n",
      "Epoch [1289/2000], Loss: 1.5069\n",
      "Epoch [1290/2000], Loss: 1.5072\n",
      "Epoch [1291/2000], Loss: 1.5062\n",
      "Epoch [1292/2000], Loss: 1.5061\n",
      "Epoch [1293/2000], Loss: 1.5055\n",
      "Epoch [1294/2000], Loss: 1.5050\n",
      "Epoch [1295/2000], Loss: 1.5076\n",
      "Epoch [1296/2000], Loss: 1.5047\n",
      "Epoch [1297/2000], Loss: 1.5045\n",
      "Epoch [1298/2000], Loss: 1.5058\n",
      "Epoch [1299/2000], Loss: 1.5026\n",
      "Epoch [1300/2000], Loss: 1.5042\n",
      "Epoch [1301/2000], Loss: 1.5065\n",
      "Epoch [1302/2000], Loss: 1.5016\n",
      "Epoch [1303/2000], Loss: 1.5036\n",
      "Epoch [1304/2000], Loss: 1.5074\n",
      "Epoch [1305/2000], Loss: 1.5004\n",
      "Epoch [1306/2000], Loss: 1.5043\n",
      "Epoch [1307/2000], Loss: 1.5063\n",
      "Epoch [1308/2000], Loss: 1.4993\n",
      "Epoch [1309/2000], Loss: 1.5053\n",
      "Epoch [1310/2000], Loss: 1.5030\n",
      "Epoch [1311/2000], Loss: 1.4987\n",
      "Epoch [1312/2000], Loss: 1.5037\n",
      "Epoch [1313/2000], Loss: 1.4992\n",
      "Epoch [1314/2000], Loss: 1.4980\n",
      "Epoch [1315/2000], Loss: 1.5001\n",
      "Epoch [1316/2000], Loss: 1.4964\n",
      "Epoch [1317/2000], Loss: 1.4966\n",
      "Epoch [1318/2000], Loss: 1.4965\n",
      "Epoch [1319/2000], Loss: 1.4941\n",
      "Epoch [1320/2000], Loss: 1.4947\n",
      "Epoch [1321/2000], Loss: 1.4941\n",
      "Epoch [1322/2000], Loss: 1.4922\n",
      "Epoch [1323/2000], Loss: 1.4928\n",
      "Epoch [1324/2000], Loss: 1.4923\n",
      "Epoch [1325/2000], Loss: 1.4909\n",
      "Epoch [1326/2000], Loss: 1.4909\n",
      "Epoch [1327/2000], Loss: 1.4904\n",
      "Epoch [1328/2000], Loss: 1.4897\n",
      "Epoch [1329/2000], Loss: 1.4897\n",
      "Epoch [1330/2000], Loss: 1.4888\n",
      "Epoch [1331/2000], Loss: 1.4885\n",
      "Epoch [1332/2000], Loss: 1.4878\n",
      "Epoch [1333/2000], Loss: 1.4880\n",
      "Epoch [1334/2000], Loss: 1.4872\n",
      "Epoch [1335/2000], Loss: 1.4868\n",
      "Epoch [1336/2000], Loss: 1.4864\n",
      "Epoch [1337/2000], Loss: 1.4865\n",
      "Epoch [1338/2000], Loss: 1.4852\n",
      "Epoch [1339/2000], Loss: 1.4851\n",
      "Epoch [1340/2000], Loss: 1.4847\n",
      "Epoch [1341/2000], Loss: 1.4842\n",
      "Epoch [1342/2000], Loss: 1.4837\n",
      "Epoch [1343/2000], Loss: 1.4834\n",
      "Epoch [1344/2000], Loss: 1.4850\n",
      "Epoch [1345/2000], Loss: 1.4834\n",
      "Epoch [1346/2000], Loss: 1.4824\n",
      "Epoch [1347/2000], Loss: 1.4852\n",
      "Epoch [1348/2000], Loss: 1.4826\n",
      "Epoch [1349/2000], Loss: 1.4819\n",
      "Epoch [1350/2000], Loss: 1.4828\n",
      "Epoch [1351/2000], Loss: 1.4803\n",
      "Epoch [1352/2000], Loss: 1.4827\n",
      "Epoch [1353/2000], Loss: 1.4831\n",
      "Epoch [1354/2000], Loss: 1.4794\n",
      "Epoch [1355/2000], Loss: 1.4821\n",
      "Epoch [1356/2000], Loss: 1.4829\n",
      "Epoch [1357/2000], Loss: 1.4781\n",
      "Epoch [1358/2000], Loss: 1.4823\n",
      "Epoch [1359/2000], Loss: 1.4809\n",
      "Epoch [1360/2000], Loss: 1.4775\n",
      "Epoch [1361/2000], Loss: 1.4808\n",
      "Epoch [1362/2000], Loss: 1.4785\n",
      "Epoch [1363/2000], Loss: 1.4766\n",
      "Epoch [1364/2000], Loss: 1.4787\n",
      "Epoch [1365/2000], Loss: 1.4751\n",
      "Epoch [1366/2000], Loss: 1.4756\n",
      "Epoch [1367/2000], Loss: 1.4756\n",
      "Epoch [1368/2000], Loss: 1.4738\n",
      "Epoch [1369/2000], Loss: 1.4732\n",
      "Epoch [1370/2000], Loss: 1.4739\n",
      "Epoch [1371/2000], Loss: 1.4717\n",
      "Epoch [1372/2000], Loss: 1.4725\n",
      "Epoch [1373/2000], Loss: 1.4716\n",
      "Epoch [1374/2000], Loss: 1.4713\n",
      "Epoch [1375/2000], Loss: 1.4746\n",
      "Epoch [1376/2000], Loss: 1.4706\n",
      "Epoch [1377/2000], Loss: 1.4723\n",
      "Epoch [1378/2000], Loss: 1.4734\n",
      "Epoch [1379/2000], Loss: 1.4686\n",
      "Epoch [1380/2000], Loss: 1.4737\n",
      "Epoch [1381/2000], Loss: 1.4722\n",
      "Epoch [1382/2000], Loss: 1.4681\n",
      "Epoch [1383/2000], Loss: 1.4728\n",
      "Epoch [1384/2000], Loss: 1.4696\n",
      "Epoch [1385/2000], Loss: 1.4676\n",
      "Epoch [1386/2000], Loss: 1.4703\n",
      "Epoch [1387/2000], Loss: 1.4671\n",
      "Epoch [1388/2000], Loss: 1.4664\n",
      "Epoch [1389/2000], Loss: 1.4677\n",
      "Epoch [1390/2000], Loss: 1.4643\n",
      "Epoch [1391/2000], Loss: 1.4648\n",
      "Epoch [1392/2000], Loss: 1.4643\n",
      "Epoch [1393/2000], Loss: 1.4634\n",
      "Epoch [1394/2000], Loss: 1.4626\n",
      "Epoch [1395/2000], Loss: 1.4632\n",
      "Epoch [1396/2000], Loss: 1.4614\n",
      "Epoch [1397/2000], Loss: 1.4616\n",
      "Epoch [1398/2000], Loss: 1.4607\n",
      "Epoch [1399/2000], Loss: 1.4609\n",
      "Epoch [1400/2000], Loss: 1.4605\n",
      "Epoch [1401/2000], Loss: 1.4600\n",
      "Epoch [1402/2000], Loss: 1.4593\n",
      "Epoch [1403/2000], Loss: 1.4590\n",
      "Epoch [1404/2000], Loss: 1.4583\n",
      "Epoch [1405/2000], Loss: 1.4583\n",
      "Epoch [1406/2000], Loss: 1.4582\n",
      "Epoch [1407/2000], Loss: 1.4574\n",
      "Epoch [1408/2000], Loss: 1.4567\n",
      "Epoch [1409/2000], Loss: 1.4564\n",
      "Epoch [1410/2000], Loss: 1.4560\n",
      "Epoch [1411/2000], Loss: 1.4558\n",
      "Epoch [1412/2000], Loss: 1.4553\n",
      "Epoch [1413/2000], Loss: 1.4544\n",
      "Epoch [1414/2000], Loss: 1.4545\n",
      "Epoch [1415/2000], Loss: 1.4543\n",
      "Epoch [1416/2000], Loss: 1.4542\n",
      "Epoch [1417/2000], Loss: 1.4533\n",
      "Epoch [1418/2000], Loss: 1.4533\n",
      "Epoch [1419/2000], Loss: 1.4534\n",
      "Epoch [1420/2000], Loss: 1.4525\n",
      "Epoch [1421/2000], Loss: 1.4514\n",
      "Epoch [1422/2000], Loss: 1.4529\n",
      "Epoch [1423/2000], Loss: 1.4518\n",
      "Epoch [1424/2000], Loss: 1.4504\n",
      "Epoch [1425/2000], Loss: 1.4502\n",
      "Epoch [1426/2000], Loss: 1.4497\n",
      "Epoch [1427/2000], Loss: 1.4486\n",
      "Epoch [1428/2000], Loss: 1.4501\n",
      "Epoch [1429/2000], Loss: 1.4502\n",
      "Epoch [1430/2000], Loss: 1.4473\n",
      "Epoch [1431/2000], Loss: 1.4492\n",
      "Epoch [1432/2000], Loss: 1.4510\n",
      "Epoch [1433/2000], Loss: 1.4465\n",
      "Epoch [1434/2000], Loss: 1.4499\n",
      "Epoch [1435/2000], Loss: 1.4489\n",
      "Epoch [1436/2000], Loss: 1.4451\n",
      "Epoch [1437/2000], Loss: 1.4495\n",
      "Epoch [1438/2000], Loss: 1.4483\n",
      "Epoch [1439/2000], Loss: 1.4446\n",
      "Epoch [1440/2000], Loss: 1.4490\n",
      "Epoch [1441/2000], Loss: 1.4451\n",
      "Epoch [1442/2000], Loss: 1.4438\n",
      "Epoch [1443/2000], Loss: 1.4462\n",
      "Epoch [1444/2000], Loss: 1.4432\n",
      "Epoch [1445/2000], Loss: 1.4427\n",
      "Epoch [1446/2000], Loss: 1.4438\n",
      "Epoch [1447/2000], Loss: 1.4406\n",
      "Epoch [1448/2000], Loss: 1.4413\n",
      "Epoch [1449/2000], Loss: 1.4409\n",
      "Epoch [1450/2000], Loss: 1.4397\n",
      "Epoch [1451/2000], Loss: 1.4391\n",
      "Epoch [1452/2000], Loss: 1.4388\n",
      "Epoch [1453/2000], Loss: 1.4382\n",
      "Epoch [1454/2000], Loss: 1.4377\n",
      "Epoch [1455/2000], Loss: 1.4383\n",
      "Epoch [1456/2000], Loss: 1.4372\n",
      "Epoch [1457/2000], Loss: 1.4363\n",
      "Epoch [1458/2000], Loss: 1.4368\n",
      "Epoch [1459/2000], Loss: 1.4369\n",
      "Epoch [1460/2000], Loss: 1.4349\n",
      "Epoch [1461/2000], Loss: 1.4360\n",
      "Epoch [1462/2000], Loss: 1.4376\n",
      "Epoch [1463/2000], Loss: 1.4340\n",
      "Epoch [1464/2000], Loss: 1.4363\n",
      "Epoch [1465/2000], Loss: 1.4356\n",
      "Epoch [1466/2000], Loss: 1.4325\n",
      "Epoch [1467/2000], Loss: 1.4361\n",
      "Epoch [1468/2000], Loss: 1.4355\n",
      "Epoch [1469/2000], Loss: 1.4317\n",
      "Epoch [1470/2000], Loss: 1.4358\n",
      "Epoch [1471/2000], Loss: 1.4343\n",
      "Epoch [1472/2000], Loss: 1.4309\n",
      "Epoch [1473/2000], Loss: 1.4348\n",
      "Epoch [1474/2000], Loss: 1.4328\n",
      "Epoch [1475/2000], Loss: 1.4301\n",
      "Epoch [1476/2000], Loss: 1.4336\n",
      "Epoch [1477/2000], Loss: 1.4292\n",
      "Epoch [1478/2000], Loss: 1.4298\n",
      "Epoch [1479/2000], Loss: 1.4311\n",
      "Epoch [1480/2000], Loss: 1.4279\n",
      "Epoch [1481/2000], Loss: 1.4278\n",
      "Epoch [1482/2000], Loss: 1.4284\n",
      "Epoch [1483/2000], Loss: 1.4269\n",
      "Epoch [1484/2000], Loss: 1.4259\n",
      "Epoch [1485/2000], Loss: 1.4264\n",
      "Epoch [1486/2000], Loss: 1.4254\n",
      "Epoch [1487/2000], Loss: 1.4246\n",
      "Epoch [1488/2000], Loss: 1.4250\n",
      "Epoch [1489/2000], Loss: 1.4256\n",
      "Epoch [1490/2000], Loss: 1.4236\n",
      "Epoch [1491/2000], Loss: 1.4247\n",
      "Epoch [1492/2000], Loss: 1.4254\n",
      "Epoch [1493/2000], Loss: 1.4223\n",
      "Epoch [1494/2000], Loss: 1.4248\n",
      "Epoch [1495/2000], Loss: 1.4235\n",
      "Epoch [1496/2000], Loss: 1.4214\n",
      "Epoch [1497/2000], Loss: 1.4244\n",
      "Epoch [1498/2000], Loss: 1.4230\n",
      "Epoch [1499/2000], Loss: 1.4204\n",
      "Epoch [1500/2000], Loss: 1.4231\n",
      "Epoch [1501/2000], Loss: 1.4217\n",
      "Epoch [1502/2000], Loss: 1.4195\n",
      "Epoch [1503/2000], Loss: 1.4220\n",
      "Epoch [1504/2000], Loss: 1.4191\n",
      "Epoch [1505/2000], Loss: 1.4185\n",
      "Epoch [1506/2000], Loss: 1.4198\n",
      "Epoch [1507/2000], Loss: 1.4183\n",
      "Epoch [1508/2000], Loss: 1.4172\n",
      "Epoch [1509/2000], Loss: 1.4185\n",
      "Epoch [1510/2000], Loss: 1.4176\n",
      "Epoch [1511/2000], Loss: 1.4160\n",
      "Epoch [1512/2000], Loss: 1.4174\n",
      "Epoch [1513/2000], Loss: 1.4155\n",
      "Epoch [1514/2000], Loss: 1.4147\n",
      "Epoch [1515/2000], Loss: 1.4160\n",
      "Epoch [1516/2000], Loss: 1.4154\n",
      "Epoch [1517/2000], Loss: 1.4136\n",
      "Epoch [1518/2000], Loss: 1.4153\n",
      "Epoch [1519/2000], Loss: 1.4152\n",
      "Epoch [1520/2000], Loss: 1.4126\n",
      "Epoch [1521/2000], Loss: 1.4148\n",
      "Epoch [1522/2000], Loss: 1.4131\n",
      "Epoch [1523/2000], Loss: 1.4116\n",
      "Epoch [1524/2000], Loss: 1.4141\n",
      "Epoch [1525/2000], Loss: 1.4126\n",
      "Epoch [1526/2000], Loss: 1.4103\n",
      "Epoch [1527/2000], Loss: 1.4131\n",
      "Epoch [1528/2000], Loss: 1.4114\n",
      "Epoch [1529/2000], Loss: 1.4097\n",
      "Epoch [1530/2000], Loss: 1.4117\n",
      "Epoch [1531/2000], Loss: 1.4086\n",
      "Epoch [1532/2000], Loss: 1.4089\n",
      "Epoch [1533/2000], Loss: 1.4097\n",
      "Epoch [1534/2000], Loss: 1.4083\n",
      "Epoch [1535/2000], Loss: 1.4071\n",
      "Epoch [1536/2000], Loss: 1.4084\n",
      "Epoch [1537/2000], Loss: 1.4076\n",
      "Epoch [1538/2000], Loss: 1.4062\n",
      "Epoch [1539/2000], Loss: 1.4073\n",
      "Epoch [1540/2000], Loss: 1.4058\n",
      "Epoch [1541/2000], Loss: 1.4048\n",
      "Epoch [1542/2000], Loss: 1.4061\n",
      "Epoch [1543/2000], Loss: 1.4056\n",
      "Epoch [1544/2000], Loss: 1.4035\n",
      "Epoch [1545/2000], Loss: 1.4049\n",
      "Epoch [1546/2000], Loss: 1.4049\n",
      "Epoch [1547/2000], Loss: 1.4029\n",
      "Epoch [1548/2000], Loss: 1.4047\n",
      "Epoch [1549/2000], Loss: 1.4030\n",
      "Epoch [1550/2000], Loss: 1.4016\n",
      "Epoch [1551/2000], Loss: 1.4034\n",
      "Epoch [1552/2000], Loss: 1.4028\n",
      "Epoch [1553/2000], Loss: 1.4006\n",
      "Epoch [1554/2000], Loss: 1.4027\n",
      "Epoch [1555/2000], Loss: 1.4016\n",
      "Epoch [1556/2000], Loss: 1.3998\n",
      "Epoch [1557/2000], Loss: 1.4018\n",
      "Epoch [1558/2000], Loss: 1.3992\n",
      "Epoch [1559/2000], Loss: 1.3986\n",
      "Epoch [1560/2000], Loss: 1.4003\n",
      "Epoch [1561/2000], Loss: 1.3989\n",
      "Epoch [1562/2000], Loss: 1.3971\n",
      "Epoch [1563/2000], Loss: 1.3989\n",
      "Epoch [1564/2000], Loss: 1.3983\n",
      "Epoch [1565/2000], Loss: 1.3964\n",
      "Epoch [1566/2000], Loss: 1.3983\n",
      "Epoch [1567/2000], Loss: 1.3963\n",
      "Epoch [1568/2000], Loss: 1.3952\n",
      "Epoch [1569/2000], Loss: 1.3968\n",
      "Epoch [1570/2000], Loss: 1.3960\n",
      "Epoch [1571/2000], Loss: 1.3941\n",
      "Epoch [1572/2000], Loss: 1.3961\n",
      "Epoch [1573/2000], Loss: 1.3952\n",
      "Epoch [1574/2000], Loss: 1.3933\n",
      "Epoch [1575/2000], Loss: 1.3957\n",
      "Epoch [1576/2000], Loss: 1.3929\n",
      "Epoch [1577/2000], Loss: 1.3924\n",
      "Epoch [1578/2000], Loss: 1.3937\n",
      "Epoch [1579/2000], Loss: 1.3925\n",
      "Epoch [1580/2000], Loss: 1.3913\n",
      "Epoch [1581/2000], Loss: 1.3928\n",
      "Epoch [1582/2000], Loss: 1.3917\n",
      "Epoch [1583/2000], Loss: 1.3903\n",
      "Epoch [1584/2000], Loss: 1.3919\n",
      "Epoch [1585/2000], Loss: 1.3896\n",
      "Epoch [1586/2000], Loss: 1.3892\n",
      "Epoch [1587/2000], Loss: 1.3904\n",
      "Epoch [1588/2000], Loss: 1.3893\n",
      "Epoch [1589/2000], Loss: 1.3876\n",
      "Epoch [1590/2000], Loss: 1.3897\n",
      "Epoch [1591/2000], Loss: 1.3890\n",
      "Epoch [1592/2000], Loss: 1.3871\n",
      "Epoch [1593/2000], Loss: 1.3889\n",
      "Epoch [1594/2000], Loss: 1.3866\n",
      "Epoch [1595/2000], Loss: 1.3860\n",
      "Epoch [1596/2000], Loss: 1.3873\n",
      "Epoch [1597/2000], Loss: 1.3867\n",
      "Epoch [1598/2000], Loss: 1.3848\n",
      "Epoch [1599/2000], Loss: 1.3868\n",
      "Epoch [1600/2000], Loss: 1.3857\n",
      "Epoch [1601/2000], Loss: 1.3842\n",
      "Epoch [1602/2000], Loss: 1.3862\n",
      "Epoch [1603/2000], Loss: 1.3836\n",
      "Epoch [1604/2000], Loss: 1.3830\n",
      "Epoch [1605/2000], Loss: 1.3844\n",
      "Epoch [1606/2000], Loss: 1.3831\n",
      "Epoch [1607/2000], Loss: 1.3815\n",
      "Epoch [1608/2000], Loss: 1.3834\n",
      "Epoch [1609/2000], Loss: 1.3825\n",
      "Epoch [1610/2000], Loss: 1.3809\n",
      "Epoch [1611/2000], Loss: 1.3826\n",
      "Epoch [1612/2000], Loss: 1.3805\n",
      "Epoch [1613/2000], Loss: 1.3797\n",
      "Epoch [1614/2000], Loss: 1.3812\n",
      "Epoch [1615/2000], Loss: 1.3804\n",
      "Epoch [1616/2000], Loss: 1.3788\n",
      "Epoch [1617/2000], Loss: 1.3807\n",
      "Epoch [1618/2000], Loss: 1.3798\n",
      "Epoch [1619/2000], Loss: 1.3780\n",
      "Epoch [1620/2000], Loss: 1.3802\n",
      "Epoch [1621/2000], Loss: 1.3773\n",
      "Epoch [1622/2000], Loss: 1.3770\n",
      "Epoch [1623/2000], Loss: 1.3785\n",
      "Epoch [1624/2000], Loss: 1.3771\n",
      "Epoch [1625/2000], Loss: 1.3755\n",
      "Epoch [1626/2000], Loss: 1.3777\n",
      "Epoch [1627/2000], Loss: 1.3750\n",
      "Epoch [1628/2000], Loss: 1.3751\n",
      "Epoch [1629/2000], Loss: 1.3760\n",
      "Epoch [1630/2000], Loss: 1.3748\n",
      "Epoch [1631/2000], Loss: 1.3738\n",
      "Epoch [1632/2000], Loss: 1.3749\n",
      "Epoch [1633/2000], Loss: 1.3729\n",
      "Epoch [1634/2000], Loss: 1.3729\n",
      "Epoch [1635/2000], Loss: 1.3732\n",
      "Epoch [1636/2000], Loss: 1.3713\n",
      "Epoch [1637/2000], Loss: 1.3717\n",
      "Epoch [1638/2000], Loss: 1.3720\n",
      "Epoch [1639/2000], Loss: 1.3715\n",
      "Epoch [1640/2000], Loss: 1.3707\n",
      "Epoch [1641/2000], Loss: 1.3701\n",
      "Epoch [1642/2000], Loss: 1.3692\n",
      "Epoch [1643/2000], Loss: 1.3699\n",
      "Epoch [1644/2000], Loss: 1.3686\n",
      "Epoch [1645/2000], Loss: 1.3685\n",
      "Epoch [1646/2000], Loss: 1.3684\n",
      "Epoch [1647/2000], Loss: 1.3676\n",
      "Epoch [1648/2000], Loss: 1.3679\n",
      "Epoch [1649/2000], Loss: 1.3700\n",
      "Epoch [1650/2000], Loss: 1.3667\n",
      "Epoch [1651/2000], Loss: 1.3691\n",
      "Epoch [1652/2000], Loss: 1.3720\n",
      "Epoch [1653/2000], Loss: 1.3660\n",
      "Epoch [1654/2000], Loss: 1.3683\n",
      "Epoch [1655/2000], Loss: 1.3682\n",
      "Epoch [1656/2000], Loss: 1.3656\n",
      "Epoch [1657/2000], Loss: 1.3677\n",
      "Epoch [1658/2000], Loss: 1.3673\n",
      "Epoch [1659/2000], Loss: 1.3642\n",
      "Epoch [1660/2000], Loss: 1.3681\n",
      "Epoch [1661/2000], Loss: 1.3660\n",
      "Epoch [1662/2000], Loss: 1.3637\n",
      "Epoch [1663/2000], Loss: 1.3667\n",
      "Epoch [1664/2000], Loss: 1.3628\n",
      "Epoch [1665/2000], Loss: 1.3635\n",
      "Epoch [1666/2000], Loss: 1.3635\n",
      "Epoch [1667/2000], Loss: 1.3621\n",
      "Epoch [1668/2000], Loss: 1.3616\n",
      "Epoch [1669/2000], Loss: 1.3623\n",
      "Epoch [1670/2000], Loss: 1.3601\n",
      "Epoch [1671/2000], Loss: 1.3603\n",
      "Epoch [1672/2000], Loss: 1.3605\n",
      "Epoch [1673/2000], Loss: 1.3589\n",
      "Epoch [1674/2000], Loss: 1.3589\n",
      "Epoch [1675/2000], Loss: 1.3596\n",
      "Epoch [1676/2000], Loss: 1.3598\n",
      "Epoch [1677/2000], Loss: 1.3581\n",
      "Epoch [1678/2000], Loss: 1.3592\n",
      "Epoch [1679/2000], Loss: 1.3603\n",
      "Epoch [1680/2000], Loss: 1.3579\n",
      "Epoch [1681/2000], Loss: 1.3578\n",
      "Epoch [1682/2000], Loss: 1.3581\n",
      "Epoch [1683/2000], Loss: 1.3569\n",
      "Epoch [1684/2000], Loss: 1.3574\n",
      "Epoch [1685/2000], Loss: 1.3565\n",
      "Epoch [1686/2000], Loss: 1.3554\n",
      "Epoch [1687/2000], Loss: 1.3557\n",
      "Epoch [1688/2000], Loss: 1.3546\n",
      "Epoch [1689/2000], Loss: 1.3544\n",
      "Epoch [1690/2000], Loss: 1.3546\n",
      "Epoch [1691/2000], Loss: 1.3557\n",
      "Epoch [1692/2000], Loss: 1.3540\n",
      "Epoch [1693/2000], Loss: 1.3549\n",
      "Epoch [1694/2000], Loss: 1.3539\n",
      "Epoch [1695/2000], Loss: 1.3527\n",
      "Epoch [1696/2000], Loss: 1.3539\n",
      "Epoch [1697/2000], Loss: 1.3520\n",
      "Epoch [1698/2000], Loss: 1.3516\n",
      "Epoch [1699/2000], Loss: 1.3536\n",
      "Epoch [1700/2000], Loss: 1.3529\n",
      "Epoch [1701/2000], Loss: 1.3505\n",
      "Epoch [1702/2000], Loss: 1.3528\n",
      "Epoch [1703/2000], Loss: 1.3532\n",
      "Epoch [1704/2000], Loss: 1.3502\n",
      "Epoch [1705/2000], Loss: 1.3512\n",
      "Epoch [1706/2000], Loss: 1.3493\n",
      "Epoch [1707/2000], Loss: 1.3492\n",
      "Epoch [1708/2000], Loss: 1.3489\n",
      "Epoch [1709/2000], Loss: 1.3480\n",
      "Epoch [1710/2000], Loss: 1.3479\n",
      "Epoch [1711/2000], Loss: 1.3482\n",
      "Epoch [1712/2000], Loss: 1.3474\n",
      "Epoch [1713/2000], Loss: 1.3465\n",
      "Epoch [1714/2000], Loss: 1.3474\n",
      "Epoch [1715/2000], Loss: 1.3477\n",
      "Epoch [1716/2000], Loss: 1.3460\n",
      "Epoch [1717/2000], Loss: 1.3472\n",
      "Epoch [1718/2000], Loss: 1.3465\n",
      "Epoch [1719/2000], Loss: 1.3448\n",
      "Epoch [1720/2000], Loss: 1.3464\n",
      "Epoch [1721/2000], Loss: 1.3447\n",
      "Epoch [1722/2000], Loss: 1.3439\n",
      "Epoch [1723/2000], Loss: 1.3459\n",
      "Epoch [1724/2000], Loss: 1.3455\n",
      "Epoch [1725/2000], Loss: 1.3428\n",
      "Epoch [1726/2000], Loss: 1.3437\n",
      "Epoch [1727/2000], Loss: 1.3432\n",
      "Epoch [1728/2000], Loss: 1.3424\n",
      "Epoch [1729/2000], Loss: 1.3421\n",
      "Epoch [1730/2000], Loss: 1.3417\n",
      "Epoch [1731/2000], Loss: 1.3413\n",
      "Epoch [1732/2000], Loss: 1.3409\n",
      "Epoch [1733/2000], Loss: 1.3402\n",
      "Epoch [1734/2000], Loss: 1.3403\n",
      "Epoch [1735/2000], Loss: 1.3406\n",
      "Epoch [1736/2000], Loss: 1.3406\n",
      "Epoch [1737/2000], Loss: 1.3390\n",
      "Epoch [1738/2000], Loss: 1.3399\n",
      "Epoch [1739/2000], Loss: 1.3411\n",
      "Epoch [1740/2000], Loss: 1.3382\n",
      "Epoch [1741/2000], Loss: 1.3388\n",
      "Epoch [1742/2000], Loss: 1.3378\n",
      "Epoch [1743/2000], Loss: 1.3372\n",
      "Epoch [1744/2000], Loss: 1.3368\n",
      "Epoch [1745/2000], Loss: 1.3364\n",
      "Epoch [1746/2000], Loss: 1.3367\n",
      "Epoch [1747/2000], Loss: 1.3364\n",
      "Epoch [1748/2000], Loss: 1.3355\n",
      "Epoch [1749/2000], Loss: 1.3354\n",
      "Epoch [1750/2000], Loss: 1.3350\n",
      "Epoch [1751/2000], Loss: 1.3346\n",
      "Epoch [1752/2000], Loss: 1.3347\n",
      "Epoch [1753/2000], Loss: 1.3339\n",
      "Epoch [1754/2000], Loss: 1.3339\n",
      "Epoch [1755/2000], Loss: 1.3331\n",
      "Epoch [1756/2000], Loss: 1.3330\n",
      "Epoch [1757/2000], Loss: 1.3331\n",
      "Epoch [1758/2000], Loss: 1.3321\n",
      "Epoch [1759/2000], Loss: 1.3328\n",
      "Epoch [1760/2000], Loss: 1.3320\n",
      "Epoch [1761/2000], Loss: 1.3317\n",
      "Epoch [1762/2000], Loss: 1.3313\n",
      "Epoch [1763/2000], Loss: 1.3305\n",
      "Epoch [1764/2000], Loss: 1.3311\n",
      "Epoch [1765/2000], Loss: 1.3305\n",
      "Epoch [1766/2000], Loss: 1.3297\n",
      "Epoch [1767/2000], Loss: 1.3294\n",
      "Epoch [1768/2000], Loss: 1.3293\n",
      "Epoch [1769/2000], Loss: 1.3299\n",
      "Epoch [1770/2000], Loss: 1.3289\n",
      "Epoch [1771/2000], Loss: 1.3287\n",
      "Epoch [1772/2000], Loss: 1.3290\n",
      "Epoch [1773/2000], Loss: 1.3274\n",
      "Epoch [1774/2000], Loss: 1.3278\n",
      "Epoch [1775/2000], Loss: 1.3269\n",
      "Epoch [1776/2000], Loss: 1.3269\n",
      "Epoch [1777/2000], Loss: 1.3277\n",
      "Epoch [1778/2000], Loss: 1.3266\n",
      "Epoch [1779/2000], Loss: 1.3268\n",
      "Epoch [1780/2000], Loss: 1.3267\n",
      "Epoch [1781/2000], Loss: 1.3257\n",
      "Epoch [1782/2000], Loss: 1.3259\n",
      "Epoch [1783/2000], Loss: 1.3249\n",
      "Epoch [1784/2000], Loss: 1.3247\n",
      "Epoch [1785/2000], Loss: 1.3246\n",
      "Epoch [1786/2000], Loss: 1.3263\n",
      "Epoch [1787/2000], Loss: 1.3246\n",
      "Epoch [1788/2000], Loss: 1.3240\n",
      "Epoch [1789/2000], Loss: 1.3252\n",
      "Epoch [1790/2000], Loss: 1.3237\n",
      "Epoch [1791/2000], Loss: 1.3225\n",
      "Epoch [1792/2000], Loss: 1.3228\n",
      "Epoch [1793/2000], Loss: 1.3217\n",
      "Epoch [1794/2000], Loss: 1.3217\n",
      "Epoch [1795/2000], Loss: 1.3224\n",
      "Epoch [1796/2000], Loss: 1.3213\n",
      "Epoch [1797/2000], Loss: 1.3210\n",
      "Epoch [1798/2000], Loss: 1.3202\n",
      "Epoch [1799/2000], Loss: 1.3207\n",
      "Epoch [1800/2000], Loss: 1.3199\n",
      "Epoch [1801/2000], Loss: 1.3200\n",
      "Epoch [1802/2000], Loss: 1.3188\n",
      "Epoch [1803/2000], Loss: 1.3183\n",
      "Epoch [1804/2000], Loss: 1.3190\n",
      "Epoch [1805/2000], Loss: 1.3190\n",
      "Epoch [1806/2000], Loss: 1.3179\n",
      "Epoch [1807/2000], Loss: 1.3179\n",
      "Epoch [1808/2000], Loss: 1.3182\n",
      "Epoch [1809/2000], Loss: 1.3165\n",
      "Epoch [1810/2000], Loss: 1.3183\n",
      "Epoch [1811/2000], Loss: 1.3177\n",
      "Epoch [1812/2000], Loss: 1.3176\n",
      "Epoch [1813/2000], Loss: 1.3159\n",
      "Epoch [1814/2000], Loss: 1.3173\n",
      "Epoch [1815/2000], Loss: 1.3183\n",
      "Epoch [1816/2000], Loss: 1.3157\n",
      "Epoch [1817/2000], Loss: 1.3157\n",
      "Epoch [1818/2000], Loss: 1.3169\n",
      "Epoch [1819/2000], Loss: 1.3148\n",
      "Epoch [1820/2000], Loss: 1.3157\n",
      "Epoch [1821/2000], Loss: 1.3188\n",
      "Epoch [1822/2000], Loss: 1.3137\n",
      "Epoch [1823/2000], Loss: 1.3173\n",
      "Epoch [1824/2000], Loss: 1.3207\n",
      "Epoch [1825/2000], Loss: 1.3131\n",
      "Epoch [1826/2000], Loss: 1.3161\n",
      "Epoch [1827/2000], Loss: 1.3165\n",
      "Epoch [1828/2000], Loss: 1.3123\n",
      "Epoch [1829/2000], Loss: 1.3168\n",
      "Epoch [1830/2000], Loss: 1.3167\n",
      "Epoch [1831/2000], Loss: 1.3115\n",
      "Epoch [1832/2000], Loss: 1.3180\n",
      "Epoch [1833/2000], Loss: 1.3155\n",
      "Epoch [1834/2000], Loss: 1.3108\n",
      "Epoch [1835/2000], Loss: 1.3174\n",
      "Epoch [1836/2000], Loss: 1.3136\n",
      "Epoch [1837/2000], Loss: 1.3106\n",
      "Epoch [1838/2000], Loss: 1.3157\n",
      "Epoch [1839/2000], Loss: 1.3097\n",
      "Epoch [1840/2000], Loss: 1.3099\n",
      "Epoch [1841/2000], Loss: 1.3129\n",
      "Epoch [1842/2000], Loss: 1.3087\n",
      "Epoch [1843/2000], Loss: 1.3081\n",
      "Epoch [1844/2000], Loss: 1.3108\n",
      "Epoch [1845/2000], Loss: 1.3081\n",
      "Epoch [1846/2000], Loss: 1.3067\n",
      "Epoch [1847/2000], Loss: 1.3075\n",
      "Epoch [1848/2000], Loss: 1.3065\n",
      "Epoch [1849/2000], Loss: 1.3061\n",
      "Epoch [1850/2000], Loss: 1.3068\n",
      "Epoch [1851/2000], Loss: 1.3074\n",
      "Epoch [1852/2000], Loss: 1.3054\n",
      "Epoch [1853/2000], Loss: 1.3070\n",
      "Epoch [1854/2000], Loss: 1.3061\n",
      "Epoch [1855/2000], Loss: 1.3044\n",
      "Epoch [1856/2000], Loss: 1.3053\n",
      "Epoch [1857/2000], Loss: 1.3031\n",
      "Epoch [1858/2000], Loss: 1.3030\n",
      "Epoch [1859/2000], Loss: 1.3050\n",
      "Epoch [1860/2000], Loss: 1.3038\n",
      "Epoch [1861/2000], Loss: 1.3024\n",
      "Epoch [1862/2000], Loss: 1.3027\n",
      "Epoch [1863/2000], Loss: 1.3017\n",
      "Epoch [1864/2000], Loss: 1.3013\n",
      "Epoch [1865/2000], Loss: 1.3013\n",
      "Epoch [1866/2000], Loss: 1.3013\n",
      "Epoch [1867/2000], Loss: 1.3005\n",
      "Epoch [1868/2000], Loss: 1.2995\n",
      "Epoch [1869/2000], Loss: 1.3013\n",
      "Epoch [1870/2000], Loss: 1.3034\n",
      "Epoch [1871/2000], Loss: 1.2998\n",
      "Epoch [1872/2000], Loss: 1.2990\n",
      "Epoch [1873/2000], Loss: 1.2993\n",
      "Epoch [1874/2000], Loss: 1.2988\n",
      "Epoch [1875/2000], Loss: 1.2977\n",
      "Epoch [1876/2000], Loss: 1.2983\n",
      "Epoch [1877/2000], Loss: 1.2978\n",
      "Epoch [1878/2000], Loss: 1.2972\n",
      "Epoch [1879/2000], Loss: 1.2985\n",
      "Epoch [1880/2000], Loss: 1.2977\n",
      "Epoch [1881/2000], Loss: 1.2970\n",
      "Epoch [1882/2000], Loss: 1.2973\n",
      "Epoch [1883/2000], Loss: 1.2961\n",
      "Epoch [1884/2000], Loss: 1.2961\n",
      "Epoch [1885/2000], Loss: 1.2966\n",
      "Epoch [1886/2000], Loss: 1.2953\n",
      "Epoch [1887/2000], Loss: 1.2966\n",
      "Epoch [1888/2000], Loss: 1.2949\n",
      "Epoch [1889/2000], Loss: 1.2946\n",
      "Epoch [1890/2000], Loss: 1.2952\n",
      "Epoch [1891/2000], Loss: 1.2950\n",
      "Epoch [1892/2000], Loss: 1.2933\n",
      "Epoch [1893/2000], Loss: 1.2928\n",
      "Epoch [1894/2000], Loss: 1.2938\n",
      "Epoch [1895/2000], Loss: 1.2952\n",
      "Epoch [1896/2000], Loss: 1.2931\n",
      "Epoch [1897/2000], Loss: 1.2918\n",
      "Epoch [1898/2000], Loss: 1.2923\n",
      "Epoch [1899/2000], Loss: 1.2921\n",
      "Epoch [1900/2000], Loss: 1.2909\n",
      "Epoch [1901/2000], Loss: 1.2915\n",
      "Epoch [1902/2000], Loss: 1.2919\n",
      "Epoch [1903/2000], Loss: 1.2908\n",
      "Epoch [1904/2000], Loss: 1.2903\n",
      "Epoch [1905/2000], Loss: 1.2899\n",
      "Epoch [1906/2000], Loss: 1.2900\n",
      "Epoch [1907/2000], Loss: 1.2891\n",
      "Epoch [1908/2000], Loss: 1.2889\n",
      "Epoch [1909/2000], Loss: 1.2895\n",
      "Epoch [1910/2000], Loss: 1.2892\n",
      "Epoch [1911/2000], Loss: 1.2890\n",
      "Epoch [1912/2000], Loss: 1.2879\n",
      "Epoch [1913/2000], Loss: 1.2882\n",
      "Epoch [1914/2000], Loss: 1.2885\n",
      "Epoch [1915/2000], Loss: 1.2874\n",
      "Epoch [1916/2000], Loss: 1.2864\n",
      "Epoch [1917/2000], Loss: 1.2861\n",
      "Epoch [1918/2000], Loss: 1.2864\n",
      "Epoch [1919/2000], Loss: 1.2881\n",
      "Epoch [1920/2000], Loss: 1.2865\n",
      "Epoch [1921/2000], Loss: 1.2850\n",
      "Epoch [1922/2000], Loss: 1.2855\n",
      "Epoch [1923/2000], Loss: 1.2855\n",
      "Epoch [1924/2000], Loss: 1.2844\n",
      "Epoch [1925/2000], Loss: 1.2848\n",
      "Epoch [1926/2000], Loss: 1.2855\n",
      "Epoch [1927/2000], Loss: 1.2842\n",
      "Epoch [1928/2000], Loss: 1.2839\n",
      "Epoch [1929/2000], Loss: 1.2831\n",
      "Epoch [1930/2000], Loss: 1.2834\n",
      "Epoch [1931/2000], Loss: 1.2828\n",
      "Epoch [1932/2000], Loss: 1.2823\n",
      "Epoch [1933/2000], Loss: 1.2826\n",
      "Epoch [1934/2000], Loss: 1.2831\n",
      "Epoch [1935/2000], Loss: 1.2829\n",
      "Epoch [1936/2000], Loss: 1.2815\n",
      "Epoch [1937/2000], Loss: 1.2815\n",
      "Epoch [1938/2000], Loss: 1.2827\n",
      "Epoch [1939/2000], Loss: 1.2813\n",
      "Epoch [1940/2000], Loss: 1.2798\n",
      "Epoch [1941/2000], Loss: 1.2797\n",
      "Epoch [1942/2000], Loss: 1.2801\n",
      "Epoch [1943/2000], Loss: 1.2815\n",
      "Epoch [1944/2000], Loss: 1.2799\n",
      "Epoch [1945/2000], Loss: 1.2785\n",
      "Epoch [1946/2000], Loss: 1.2788\n",
      "Epoch [1947/2000], Loss: 1.2791\n",
      "Epoch [1948/2000], Loss: 1.2781\n",
      "Epoch [1949/2000], Loss: 1.2773\n",
      "Epoch [1950/2000], Loss: 1.2770\n",
      "Epoch [1951/2000], Loss: 1.2766\n",
      "Epoch [1952/2000], Loss: 1.2777\n",
      "Epoch [1953/2000], Loss: 1.2776\n",
      "Epoch [1954/2000], Loss: 1.2769\n",
      "Epoch [1955/2000], Loss: 1.2768\n",
      "Epoch [1956/2000], Loss: 1.2764\n",
      "Epoch [1957/2000], Loss: 1.2763\n",
      "Epoch [1958/2000], Loss: 1.2759\n",
      "Epoch [1959/2000], Loss: 1.2760\n",
      "Epoch [1960/2000], Loss: 1.2778\n",
      "Epoch [1961/2000], Loss: 1.2765\n",
      "Epoch [1962/2000], Loss: 1.2754\n",
      "Epoch [1963/2000], Loss: 1.2742\n",
      "Epoch [1964/2000], Loss: 1.2753\n",
      "Epoch [1965/2000], Loss: 1.2747\n",
      "Epoch [1966/2000], Loss: 1.2739\n",
      "Epoch [1967/2000], Loss: 1.2744\n",
      "Epoch [1968/2000], Loss: 1.2749\n",
      "Epoch [1969/2000], Loss: 1.2740\n",
      "Epoch [1970/2000], Loss: 1.2727\n",
      "Epoch [1971/2000], Loss: 1.2729\n",
      "Epoch [1972/2000], Loss: 1.2734\n",
      "Epoch [1973/2000], Loss: 1.2732\n",
      "Epoch [1974/2000], Loss: 1.2708\n",
      "Epoch [1975/2000], Loss: 1.2727\n",
      "Epoch [1976/2000], Loss: 1.2760\n",
      "Epoch [1977/2000], Loss: 1.2726\n",
      "Epoch [1978/2000], Loss: 1.2702\n",
      "Epoch [1979/2000], Loss: 1.2714\n",
      "Epoch [1980/2000], Loss: 1.2716\n",
      "Epoch [1981/2000], Loss: 1.2714\n",
      "Epoch [1982/2000], Loss: 1.2696\n",
      "Epoch [1983/2000], Loss: 1.2695\n",
      "Epoch [1984/2000], Loss: 1.2687\n",
      "Epoch [1985/2000], Loss: 1.2689\n",
      "Epoch [1986/2000], Loss: 1.2679\n",
      "Epoch [1987/2000], Loss: 1.2682\n",
      "Epoch [1988/2000], Loss: 1.2685\n",
      "Epoch [1989/2000], Loss: 1.2690\n",
      "Epoch [1990/2000], Loss: 1.2673\n",
      "Epoch [1991/2000], Loss: 1.2674\n",
      "Epoch [1992/2000], Loss: 1.2705\n",
      "Epoch [1993/2000], Loss: 1.2690\n",
      "Epoch [1994/2000], Loss: 1.2669\n",
      "Epoch [1995/2000], Loss: 1.2672\n",
      "Epoch [1996/2000], Loss: 1.2669\n",
      "Epoch [1997/2000], Loss: 1.2654\n",
      "Epoch [1998/2000], Loss: 1.2664\n",
      "Epoch [1999/2000], Loss: 1.2664\n",
      "Epoch [2000/2000], Loss: 1.2651\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "x_train_date = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_date = torch.tensor(y_train.values, dtype=torch.int64)\n",
    "\n",
    "\n",
    "# Определение архитектуры нейронной сети\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)  # Входной слой: 3 признака -> 64 нейрона\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)  # Скрытый слой: 64 нейрона -> 32 нейрона\n",
    "        self.fc3 = nn.Linear(32, 53)  # Выходной слой: 32 нейрона -> 53 выход \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Инициализация модели и функции потерь\n",
    "model = SimpleClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение модели\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_train_date)\n",
    "    loss = criterion(output, y_train_date)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Создание фиктивных входных данных\n",
    "dummy_input = torch.randn(1, 3)  # Размерность входных данных должна соответствовать вашим данным\n",
    "\n",
    "# Укажите путь для сохранения модели в формате ONNX\n",
    "onnx_path = \"metamodel.onnx\"\n",
    "\n",
    "# Экспорт модели в формат ONNX\n",
    "torch.onnx.export(model, dummy_input, onnx_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Создание исполнителя (runtime) для модели ONNX\n",
    "ort_session = onnxruntime.InferenceSession(\"./metamodel.onnx\")\n",
    "\n",
    "# Создание входных данных для предсказания (данные должны соответствовать формату, используемому при обучении модели)\n",
    "input_data = np.array([[10, 20, 30]], dtype=np.float32)\n",
    "  # Замените ... на ваши входные данные\n",
    "\n",
    "# Предсказание\n",
    "outputs = ort_session.run(None, {\"onnx::Gemm_0\": input_data})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-25.41567   , -12.921899  , -84.79633   , -64.49079   ,\n",
       "         -70.72872   , -29.752794  ,  -3.3705416 ,  15.261076  ,\n",
       "          -0.90996516,  30.905178  ,  -6.007939  , -84.946754  ,\n",
       "         -12.913983  , -16.98314   , -22.282791  ,  -7.9085274 ,\n",
       "         -21.395092  ,  20.0179    ,  27.931566  ,  32.618572  ,\n",
       "          40.36392   ,  -8.757456  ,  17.811491  , -12.739071  ,\n",
       "         -12.8525    ,  35.48876   ,  43.322754  ,  13.947449  ,\n",
       "           6.25141   ,  -8.6482725 ,  45.287987  ,  -7.3162494 ,\n",
       "          17.354692  ,  42.89952   , -25.465046  ,  32.4281    ,\n",
       "           5.9982333 , -32.366417  , -29.535975  ,  18.582813  ,\n",
       "         -23.057295  , -28.449553  , -24.052343  ,   0.9745277 ,\n",
       "         -21.569006  , -41.7246    , -36.428688  , -52.981213  ,\n",
       "           5.342592  , -43.894096  , -49.073803  , -47.70679   ,\n",
       "         -40.245296  ]], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=outputs[0]\n",
    "x=x[0]\n",
    "\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Индекс максимального числа: 30\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(x)\n",
    "print(\"Индекс максимального числа:\", max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ace of clubs',\n",
       " 'ace of diamonds',\n",
       " 'ace of hearts',\n",
       " 'ace of spades',\n",
       " 'eight of clubs',\n",
       " 'eight of diamonds',\n",
       " 'eight of hearts',\n",
       " 'eight of spades',\n",
       " 'five of clubs',\n",
       " 'five of diamonds',\n",
       " 'five of hearts',\n",
       " 'five of spades',\n",
       " 'four of clubs',\n",
       " 'four of diamonds',\n",
       " 'four of hearts',\n",
       " 'four of spades',\n",
       " 'jack of clubs',\n",
       " 'jack of diamonds',\n",
       " 'jack of hearts',\n",
       " 'jack of spades',\n",
       " 'joker',\n",
       " 'king of clubs',\n",
       " 'king of diamonds',\n",
       " 'king of hearts',\n",
       " 'king of spades',\n",
       " 'nine of clubs',\n",
       " 'nine of diamonds',\n",
       " 'nine of hearts',\n",
       " 'nine of spades',\n",
       " 'queen of clubs',\n",
       " 'queen of diamonds',\n",
       " 'queen of hearts',\n",
       " 'queen of spades',\n",
       " 'seven of clubs',\n",
       " 'seven of diamonds',\n",
       " 'seven of hearts',\n",
       " 'seven of spades',\n",
       " 'six of clubs',\n",
       " 'six of diamonds',\n",
       " 'six of hearts',\n",
       " 'six of spades',\n",
       " 'ten of clubs',\n",
       " 'ten of diamonds',\n",
       " 'ten of hearts',\n",
       " 'ten of spades',\n",
       " 'three of clubs',\n",
       " 'three of diamonds',\n",
       " 'three of hearts',\n",
       " 'three of spades',\n",
       " 'two of clubs',\n",
       " 'two of diamonds',\n",
       " 'two of hearts',\n",
       " 'two of spades']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя класса : queen of diamonds\n"
     ]
    }
   ],
   "source": [
    "print(\"Имя класса :\",class_names[max_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главнаая функция с предсказаниями всех моделей и в итоге предсказанием метамодели для итогового результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_meta(input_data):\n",
    "    ort_session = onnxruntime.InferenceSession(\"./metamodel.onnx\")\n",
    "\n",
    "    \n",
    "    input_data = np.array([[10, 20, 30]], dtype=np.float32)\n",
    "\n",
    "    outputs = ort_session.run(None, {\"onnx::Gemm_0\": input_data})\n",
    "    return outputs\n",
    "\n",
    "def predict_image(model_path, image_path, img_transforms, device):\n",
    "    # Загрузка ONNX модели\n",
    "    session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "    # Открытие изображения\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Применение преобразований к изображению\n",
    "    image = img_transforms(image).unsqueeze(0)\n",
    "\n",
    "    # Получение входного имени и выходного имени из модели\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    # Выполнение предсказания\n",
    "    input_data = image.cpu().numpy() if device == torch.device(\"cuda\") else image.numpy()\n",
    "\n",
    "    outputs = session.run([output_name], {input_name: input_data})\n",
    "\n",
    "    # Получение индекса предсказанного класса\n",
    "    predicted_class = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "    return predicted_class.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Имя класса : ace of clubs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred=[]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models=[\"mobile.onnx\",\"model.onnx\",\"squeezenet.onnx\"]\n",
    "for i in models:\n",
    "    predicted_class = predict_image(i, \"./test/ace of clubs/5.jpg\", img_transforms, device)\n",
    "    pred.append(predicted_class)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "predict_meta(pred)\n",
    "print(\"Имя класса :\",class_names[predicted_class])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод использование стекинга повышает итоговые метрики модели "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
